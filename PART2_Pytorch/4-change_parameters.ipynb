{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9a3c6b",
   "metadata": {},
   "source": [
    "# Modification des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036717bb",
   "metadata": {},
   "source": [
    "Ici, nous allons essayer d'optimiser les paramètres d'entraînement ainsi que le learning rate. Voici les caractéristiques du meilleur modèle que nous avons obtenu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d252cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {'num_convs': (3, 3), 'conv_sizes': (3, 3), 'fc_sizes': (200, 30)}\n",
    "with_dropout = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a3c4e",
   "metadata": {},
   "source": [
    "Nous allons maintenant lancer le code ci-dessous pour essayer de trouver les valeurs optimales pour le learning rate et la batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d55949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/vincent/.local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/vincent/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/vincent/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/vincent/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/vincent/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/vincent/.local/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: wheel in /home/vincent/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/vincent/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vincent/.local/lib/python3.10/site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: torch==1.13.1 in /home/vincent/.local/lib/python3.10/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/vincent/.local/lib/python3.10/site-packages (from torch==1.13.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/vincent/.local/lib/python3.10/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/vincent/.local/lib/python3.10/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/vincent/.local/lib/python3.10/site-packages (from torch==1.13.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /home/vincent/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/vincent/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (65.7.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2081b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing with GPU\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mp\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as om\n",
    "import torchvision as tv\n",
    "import torch.utils.data as dat\n",
    "\n",
    "if torch.cuda.is_available():     # Make sure GPU is available\n",
    "    print(\"Computing with GPU\")\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "    kwar = {'num_workers': 8, 'pin_memory': True}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(\"Warning: CUDA not found, CPU only.\")\n",
    "    dev = torch.device(\"cpu\")\n",
    "    kwar = {}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "\n",
    "np.random.seed(551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff7ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58954 images in 6 distinct categories\n",
      "Label names: ['ChestCT', 'CXR', 'BreastMRI', 'Hand', 'HeadCT', 'AbdomenCT']\n",
      "Label counts: [10000, 10000, 8954, 10000, 10000, 10000]\n",
      "Image dimensions: 64 x 64\n",
      "Rescaled min pixel value = -0.786; Max = 0.972; Mean = -3.33e-09\n",
      "Training images = 47230 Validation = 5878 Testing = 5846\n"
     ]
    }
   ],
   "source": [
    "class MedNet(nn.Module):\n",
    "    def __init__(self,xDim,yDim,numC, num_convs=(5, 10), conv_sizes=(7, 7), fc_sizes=(400, 80), add_dropout=False): # Pass image dimensions and number of labels when initializing a model   \n",
    "        super(MedNet,self).__init__()  # Extends the basic nn.Module to the MedNet class\n",
    "        # The parameters here define the architecture of the convolutional portion of the CNN. Each image pixel\n",
    "        # has numConvs convolutions applied to it, and convSize is the number of surrounding pixels included\n",
    "        # in each convolution. Lastly, the numNodesToFC formula calculates the final, remaining nodes at the last\n",
    "        # level of convolutions so that this can be \"flattened\" and fed into the fully connected layers subsequently.\n",
    "        # Each convolution makes the image a little smaller (convolutions do not, by default, \"hang over\" the edges\n",
    "        # of the image), and this makes the effective image dimension decreases.\n",
    "        \n",
    "        self.add_dropout = add_dropout\n",
    "        \n",
    "        numConvs1 = num_convs[0]\n",
    "        convSize1 = conv_sizes[0]\n",
    "        numConvs2 = num_convs[1]\n",
    "        convSize2 = conv_sizes[1]\n",
    "        numNodesToFC = numConvs2*(xDim-(convSize1-1)-(convSize2-1))*(yDim-(convSize1-1)-(convSize2-1))\n",
    "\n",
    "        # nn.Conv2d(channels in, channels out, convolution height/width)\n",
    "        # 1 channel -- grayscale -- feeds into the first convolution. The same number output from one layer must be\n",
    "        # fed into the next. These variables actually store the weights between layers for the model.\n",
    "        \n",
    "        self.cnv1 = nn.Conv2d(1, numConvs1, convSize1)\n",
    "        self.cnv2 = nn.Conv2d(numConvs1, numConvs2, convSize2)\n",
    "\n",
    "        # These parameters define the number of output nodes of each fully connected layer.\n",
    "        # Each layer must output the same number of nodes as the next layer begins with.\n",
    "        # The final layer must have output nodes equal to the number of labels used.\n",
    "        \n",
    "        fcSize1 = fc_sizes[0]\n",
    "        fcSize2 = fc_sizes[1]\n",
    "        \n",
    "        # nn.Linear(nodes in, nodes out)\n",
    "        # Stores the weights between the fully connected layers\n",
    "        \n",
    "        self.ful1 = nn.Linear(numNodesToFC,fcSize1)\n",
    "        if self.add_dropout: self.drop1 = nn.Dropout(0.5)\n",
    "        self.ful2 = nn.Linear(fcSize1, fcSize2)\n",
    "        if self.add_dropout: self.drop2 = nn.Dropout(0.5)\n",
    "        self.ful3 = nn.Linear(fcSize2,numC)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # This defines the steps used in the computation of output from input.\n",
    "        # It makes uses of the weights defined in the __init__ method.\n",
    "        # Each assignment of x here is the result of feeding the input up through one layer.\n",
    "        # Here we use the activation function elu, which is a smoother version of the popular relu function.\n",
    "        \n",
    "        x = F.elu(self.cnv1(x)) # Feed through first convolutional layer, then apply activation\n",
    "        x = F.elu(self.cnv2(x)) # Feed through second convolutional layer, apply activation\n",
    "        x = x.view(-1,self.num_flat_features(x)) # Flatten convolutional layer into fully connected layer\n",
    "        x = F.elu(self.ful1(x)) # Feed through first fully connected layer, apply activation\n",
    "        if self.add_dropout: x = self.drop1(x)\n",
    "        x = F.elu(self.ful2(x)) # Feed through second FC layer, apply output\n",
    "        if self.add_dropout: x = self.drop2(x)\n",
    "        x = self.ful3(x)        # Final FC layer to output. No activation, because it's used to calculate loss\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):  # Count the individual nodes in a layer\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "def train_model(params, \n",
    "                trainX, \n",
    "                trainY, \n",
    "                validX, \n",
    "                validY, \n",
    "                testX, \n",
    "                testY, \n",
    "                learnRate=0.01, \n",
    "                batchSize = 300,\n",
    "                verbose=False, \n",
    "                add_dropout=False): # params est un dictionnaire qui contient les structures de CNN à essayer\n",
    "    \n",
    "    model = MedNet(imageWidth,imageHeight,numClass, add_dropout=add_dropout, **params).to(dev)\n",
    "\n",
    "    learnRate = learnRate          # Define a learning rate.\n",
    "    maxEpochs = 200           # Maximum training epochs\n",
    "    t2vRatio = 1.2            # Maximum allowed ratio of validation to training loss\n",
    "    t2vEpochs = 3             # Number of consecutive epochs before halting if validation loss exceeds above limit\n",
    "    batchSize = batchSize          # Batch size. Going too large will cause an out-of-memory error.\n",
    "    trainBats = nTrain // batchSize       # Number of training batches per epoch. Round down to simplify last batch\n",
    "    validBats = nValid // batchSize       # Validation batches. Round down\n",
    "    testBats = -(-nTest // batchSize)     # Testing batches. Round up to include all\n",
    "    CEweights = torch.zeros(numClass)     # This takes into account the imbalanced dataset.\n",
    "    for i in trainY.tolist():             #      By making rarer images count more to the loss, \n",
    "        CEweights[i].add_(1)              #      we prevent the model from ignoring them.\n",
    "    CEweights = 1. / CEweights.clamp_(min=1.)                     # Weights should be inversely related to count\n",
    "    CEweights = (CEweights * numClass / CEweights.sum()).to(dev)  # The weights average to 1\n",
    "    opti = om.SGD(model.parameters(), lr = learnRate)   # Initialize an optimizer\n",
    "\n",
    "\n",
    "    for i in range(maxEpochs):\n",
    "        model.train()                     # Set model to training mode\n",
    "        epochLoss = 0.\n",
    "        permute = torch.randperm(nTrain)  # Shuffle data to randomize batches\n",
    "        trainX = trainX[permute,:,:,:]\n",
    "        trainY = trainY[permute]\n",
    "        for j in range(trainBats):        # Iterate over batches\n",
    "            opti.zero_grad()              # Zero out gradient accumulated in optimizer\n",
    "            batX = trainX[j*batchSize:(j+1)*batchSize,:,:,:].to(dev)   # Slice shuffled data into batches\n",
    "            batY = trainY[j*batchSize:(j+1)*batchSize].to(dev)         # .to(dev) moves these batches to the GPU\n",
    "            yOut = model(batX)            # Evalute predictions\n",
    "            loss = F.cross_entropy(yOut, batY,weight=CEweights)        # Compute loss\n",
    "            epochLoss += loss.item()      # Add loss\n",
    "            loss.backward()               # Backpropagate loss\n",
    "            opti.step()                   # Update model weights using optimizer\n",
    "        validLoss = 0.\n",
    "        permute = torch.randperm(nValid)  # We go through the exact same steps, without backprop / optimization\n",
    "        validX = validX[permute,:,:,:]    # in order to evaluate the validation loss\n",
    "        validY = validY[permute]\n",
    "        model.eval()                      # Set model to evaluation mode\n",
    "        with torch.no_grad():             # Temporarily turn off gradient descent\n",
    "            for j in range(validBats):\n",
    "                opti.zero_grad()\n",
    "                batX = validX[j*batchSize:(j+1)*batchSize,:,:,:].to(dev)\n",
    "                batY = validY[j*batchSize:(j+1)*batchSize].to(dev)\n",
    "                yOut = model(batX)\n",
    "                validLoss += F.cross_entropy(yOut, batY,weight=CEweights).item()\n",
    "        epochLoss /= trainBats            # Average loss over batches and print\n",
    "        validLoss /= validBats\n",
    "        if verbose: print(\"Epoch = {:-3}; Training loss = {:.4f}; Validation loss = {:.4f}\".format(i,epochLoss,validLoss))\n",
    "        if validLoss > t2vRatio * epochLoss:\n",
    "            t2vEpochs -= 1                # Test if validation loss exceeds halting threshold\n",
    "            if t2vEpochs < 1:\n",
    "                if verbose: print(\"Validation loss too high; halting to prevent overfitting\")\n",
    "                break\n",
    "\n",
    "    confuseMtx = np.zeros((numClass,numClass),dtype=int)    # Create empty confusion matrix\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        permute = torch.randperm(nTest)                     # Shuffle test data\n",
    "        testX = testX[permute,:,:,:]\n",
    "        testY = testY[permute]\n",
    "        for j in range(testBats):                           # Iterate over test batches\n",
    "            batX = testX[j*batchSize:(j+1)*batchSize,:,:,:].to(dev)\n",
    "            batY = testY[j*batchSize:(j+1)*batchSize].to(dev)\n",
    "            yOut = model(batX)                              # Pass test batch through model\n",
    "            pred = yOut.max(1,keepdim=True)[1]              # Generate predictions by finding the max Y values\n",
    "            for j in torch.cat((batY.view_as(pred), pred),dim=1).tolist(): # Glue together Actual and Predicted to\n",
    "                confuseMtx[j[0],j[1]] += 1                  # make (row, col) pairs, and increment confusion matrix\n",
    "    correct = sum([confuseMtx[i,i] for i in range(numClass)])   # Sum over diagonal elements to count correct predictions\n",
    "    if verbose: \n",
    "        print(f\"Correct predictions: {correct} of {nTest}. Accuracy: {correct/nTest*100}%\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confuseMtx)\n",
    "        print(classNames)\n",
    "    return (model, correct/nTest)\n",
    "\n",
    "  \n",
    "\n",
    "dataDir = 'resized'               # The main data directory\n",
    "classNames = os.listdir(dataDir)  # Each type of image can be found in its own subdirectory\n",
    "numClass = len(classNames)        # Number of types = number of subdirectories\n",
    "imageFiles = [[os.path.join(dataDir,classNames[i],x) for x in os.listdir(os.path.join(dataDir,classNames[i]))]\n",
    "              for i in range(numClass)]                     # A nested list of filenames\n",
    "numEach = [len(imageFiles[i]) for i in range(numClass)]     # A count of each type of image\n",
    "imageFilesList = []               # Created an un-nested list of filenames\n",
    "imageClass = []                   # The labels -- the type of each individual image in the list\n",
    "for i in range(numClass):\n",
    "    imageFilesList.extend(imageFiles[i])\n",
    "    imageClass.extend([i]*numEach[i])\n",
    "numTotal = len(imageClass)        # Total number of images\n",
    "imageWidth, imageHeight = Image.open(imageFilesList[0]).size         # The dimensions of each image\n",
    "\n",
    "print(\"There are\",numTotal,\"images in\",numClass,\"distinct categories\")\n",
    "print(\"Label names:\",classNames)\n",
    "print(\"Label counts:\",numEach)\n",
    "print(\"Image dimensions:\",imageWidth,\"x\",imageHeight)\n",
    "\n",
    "toTensor = tv.transforms.ToTensor()\n",
    "def scaleImage(x):          # Pass a PIL image, return a tensor\n",
    "    y = toTensor(x)\n",
    "    if(y.min() < y.max()):  # Assuming the image isn't empty, rescale so its values run from 0 to 1\n",
    "        y = (y - y.min())/(y.max() - y.min()) \n",
    "    z = y - y.mean()        # Subtract the mean value of the image\n",
    "    return z\n",
    "\n",
    "imageTensor = torch.stack([scaleImage(Image.open(x)) for x in imageFilesList])  # Load, scale, and stack image (X) tensor\n",
    "classTensor = torch.tensor(imageClass)  # Create label (Y) tensor\n",
    "print(\"Rescaled min pixel value = {:1.3}; Max = {:1.3}; Mean = {:1.3}\"\n",
    "      .format(imageTensor.min().item(),imageTensor.max().item(),imageTensor.mean().item()))\n",
    "\n",
    "validFrac = 0.1   # Define the fraction of images to move to validation dataset\n",
    "testFrac = 0.1    # Define the fraction of images to move to test dataset\n",
    "validList = []\n",
    "testList = []\n",
    "trainList = []\n",
    "\n",
    "for i in range(numTotal):\n",
    "    rann = np.random.random() # Randomly reassign images\n",
    "    if rann < validFrac:\n",
    "        validList.append(i)\n",
    "    elif rann < testFrac + validFrac:\n",
    "        testList.append(i)\n",
    "    else:\n",
    "        trainList.append(i)\n",
    "        \n",
    "nTrain = len(trainList)  # Count the number in each set\n",
    "nValid = len(validList)\n",
    "nTest = len(testList)\n",
    "print(\"Training images =\",nTrain,\"Validation =\",nValid,\"Testing =\",nTest)\n",
    "trainIds = torch.tensor(trainList)    # Slice the big image and label tensors up into\n",
    "validIds = torch.tensor(validList)    #       training, validation, and testing tensors\n",
    "testIds = torch.tensor(testList)\n",
    "trainX = imageTensor[trainIds,:,:,:]\n",
    "trainY = classTensor[trainIds]\n",
    "validX = imageTensor[validIds,:,:,:]\n",
    "validY = classTensor[validIds]\n",
    "testX = imageTensor[testIds,:,:,:]\n",
    "testY = classTensor[testIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e35063e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =   0; Training loss = 1.7757; Validation loss = 1.7275\n",
      "Epoch =   1; Training loss = 1.2879; Validation loss = 0.6153\n",
      "Epoch =   2; Training loss = 0.5264; Validation loss = 0.2926\n",
      "Epoch =   3; Training loss = 0.3555; Validation loss = 0.2138\n",
      "Epoch =   4; Training loss = 0.2862; Validation loss = 0.1718\n",
      "Epoch =   5; Training loss = 0.2368; Validation loss = 0.1419\n",
      "Epoch =   6; Training loss = 0.1994; Validation loss = 0.1176\n",
      "Epoch =   7; Training loss = 0.1788; Validation loss = 0.1018\n",
      "Epoch =   8; Training loss = 0.1558; Validation loss = 0.0874\n",
      "Epoch =   9; Training loss = 0.1351; Validation loss = 0.0767\n",
      "Epoch =  10; Training loss = 0.1251; Validation loss = 0.0682\n",
      "Epoch =  11; Training loss = 0.1149; Validation loss = 0.0619\n",
      "Epoch =  12; Training loss = 0.1032; Validation loss = 0.0553\n",
      "Epoch =  13; Training loss = 0.0965; Validation loss = 0.0504\n",
      "Epoch =  14; Training loss = 0.0867; Validation loss = 0.0461\n",
      "Epoch =  15; Training loss = 0.0828; Validation loss = 0.0410\n",
      "Epoch =  16; Training loss = 0.0768; Validation loss = 0.0381\n",
      "Epoch =  17; Training loss = 0.0699; Validation loss = 0.0347\n",
      "Epoch =  18; Training loss = 0.0656; Validation loss = 0.0310\n",
      "Epoch =  19; Training loss = 0.0621; Validation loss = 0.0290\n",
      "Epoch =  20; Training loss = 0.0590; Validation loss = 0.0260\n",
      "Epoch =  21; Training loss = 0.0541; Validation loss = 0.0243\n",
      "Epoch =  22; Training loss = 0.0507; Validation loss = 0.0223\n",
      "Epoch =  23; Training loss = 0.0478; Validation loss = 0.0207\n",
      "Epoch =  24; Training loss = 0.0456; Validation loss = 0.0194\n",
      "Epoch =  25; Training loss = 0.0447; Validation loss = 0.0175\n",
      "Epoch =  26; Training loss = 0.0422; Validation loss = 0.0178\n",
      "Epoch =  27; Training loss = 0.0400; Validation loss = 0.0167\n",
      "Epoch =  28; Training loss = 0.0390; Validation loss = 0.0160\n",
      "Epoch =  29; Training loss = 0.0367; Validation loss = 0.0144\n",
      "Epoch =  30; Training loss = 0.0338; Validation loss = 0.0139\n",
      "Epoch =  31; Training loss = 0.0342; Validation loss = 0.0137\n",
      "Epoch =  32; Training loss = 0.0316; Validation loss = 0.0137\n",
      "Epoch =  33; Training loss = 0.0315; Validation loss = 0.0128\n",
      "Epoch =  34; Training loss = 0.0300; Validation loss = 0.0120\n",
      "Epoch =  35; Training loss = 0.0299; Validation loss = 0.0118\n",
      "Epoch =  36; Training loss = 0.0273; Validation loss = 0.0113\n",
      "Epoch =  37; Training loss = 0.0275; Validation loss = 0.0107\n",
      "Epoch =  38; Training loss = 0.0258; Validation loss = 0.0105\n",
      "Epoch =  39; Training loss = 0.0253; Validation loss = 0.0109\n",
      "Epoch =  40; Training loss = 0.0243; Validation loss = 0.0110\n",
      "Epoch =  41; Training loss = 0.0250; Validation loss = 0.0115\n",
      "Epoch =  42; Training loss = 0.0249; Validation loss = 0.0105\n",
      "Epoch =  43; Training loss = 0.0222; Validation loss = 0.0099\n",
      "Epoch =  44; Training loss = 0.0211; Validation loss = 0.0092\n",
      "Epoch =  45; Training loss = 0.0223; Validation loss = 0.0096\n",
      "Epoch =  46; Training loss = 0.0211; Validation loss = 0.0095\n",
      "Epoch =  47; Training loss = 0.0216; Validation loss = 0.0094\n",
      "Epoch =  48; Training loss = 0.0198; Validation loss = 0.0094\n",
      "Epoch =  49; Training loss = 0.0199; Validation loss = 0.0086\n",
      "Epoch =  50; Training loss = 0.0192; Validation loss = 0.0091\n",
      "Epoch =  51; Training loss = 0.0173; Validation loss = 0.0085\n",
      "Epoch =  52; Training loss = 0.0193; Validation loss = 0.0079\n",
      "Epoch =  53; Training loss = 0.0179; Validation loss = 0.0085\n",
      "Epoch =  54; Training loss = 0.0183; Validation loss = 0.0089\n",
      "Epoch =  55; Training loss = 0.0175; Validation loss = 0.0078\n",
      "Epoch =  56; Training loss = 0.0169; Validation loss = 0.0086\n",
      "Epoch =  57; Training loss = 0.0166; Validation loss = 0.0094\n",
      "Epoch =  58; Training loss = 0.0162; Validation loss = 0.0083\n",
      "Epoch =  59; Training loss = 0.0156; Validation loss = 0.0092\n",
      "Epoch =  60; Training loss = 0.0148; Validation loss = 0.0081\n",
      "Epoch =  61; Training loss = 0.0149; Validation loss = 0.0086\n",
      "Epoch =  62; Training loss = 0.0140; Validation loss = 0.0082\n",
      "Epoch =  63; Training loss = 0.0144; Validation loss = 0.0084\n",
      "Epoch =  64; Training loss = 0.0142; Validation loss = 0.0080\n",
      "Epoch =  65; Training loss = 0.0138; Validation loss = 0.0078\n",
      "Epoch =  66; Training loss = 0.0137; Validation loss = 0.0080\n",
      "Epoch =  67; Training loss = 0.0136; Validation loss = 0.0085\n",
      "Epoch =  68; Training loss = 0.0127; Validation loss = 0.0073\n",
      "Epoch =  69; Training loss = 0.0135; Validation loss = 0.0075\n",
      "Epoch =  70; Training loss = 0.0121; Validation loss = 0.0077\n",
      "Epoch =  71; Training loss = 0.0125; Validation loss = 0.0076\n",
      "Epoch =  72; Training loss = 0.0123; Validation loss = 0.0074\n",
      "Epoch =  73; Training loss = 0.0125; Validation loss = 0.0076\n",
      "Epoch =  74; Training loss = 0.0125; Validation loss = 0.0073\n",
      "Epoch =  75; Training loss = 0.0119; Validation loss = 0.0074\n",
      "Epoch =  76; Training loss = 0.0118; Validation loss = 0.0074\n",
      "Epoch =  77; Training loss = 0.0113; Validation loss = 0.0078\n",
      "Epoch =  78; Training loss = 0.0112; Validation loss = 0.0075\n",
      "Epoch =  79; Training loss = 0.0112; Validation loss = 0.0069\n",
      "Epoch =  80; Training loss = 0.0106; Validation loss = 0.0066\n",
      "Epoch =  81; Training loss = 0.0107; Validation loss = 0.0075\n",
      "Epoch =  82; Training loss = 0.0103; Validation loss = 0.0075\n",
      "Epoch =  83; Training loss = 0.0096; Validation loss = 0.0074\n",
      "Epoch =  84; Training loss = 0.0105; Validation loss = 0.0070\n",
      "Epoch =  85; Training loss = 0.0102; Validation loss = 0.0073\n",
      "Epoch =  86; Training loss = 0.0102; Validation loss = 0.0082\n",
      "Epoch =  87; Training loss = 0.0098; Validation loss = 0.0070\n",
      "Epoch =  88; Training loss = 0.0108; Validation loss = 0.0073\n",
      "Epoch =  89; Training loss = 0.0093; Validation loss = 0.0076\n",
      "Epoch =  90; Training loss = 0.0094; Validation loss = 0.0079\n",
      "Epoch =  91; Training loss = 0.0093; Validation loss = 0.0071\n",
      "Epoch =  92; Training loss = 0.0088; Validation loss = 0.0074\n",
      "Epoch =  93; Training loss = 0.0085; Validation loss = 0.0072\n",
      "Epoch =  94; Training loss = 0.0089; Validation loss = 0.0077\n",
      "Epoch =  95; Training loss = 0.0086; Validation loss = 0.0074\n",
      "Epoch =  96; Training loss = 0.0090; Validation loss = 0.0075\n",
      "Epoch =  97; Training loss = 0.0087; Validation loss = 0.0074\n",
      "Epoch =  98; Training loss = 0.0086; Validation loss = 0.0076\n",
      "Epoch =  99; Training loss = 0.0086; Validation loss = 0.0075\n",
      "Epoch = 100; Training loss = 0.0089; Validation loss = 0.0086\n",
      "Epoch = 101; Training loss = 0.0081; Validation loss = 0.0070\n",
      "Epoch = 102; Training loss = 0.0077; Validation loss = 0.0079\n",
      "Epoch = 103; Training loss = 0.0083; Validation loss = 0.0073\n",
      "Epoch = 104; Training loss = 0.0078; Validation loss = 0.0069\n",
      "Epoch = 105; Training loss = 0.0078; Validation loss = 0.0068\n",
      "Epoch = 106; Training loss = 0.0080; Validation loss = 0.0070\n",
      "Epoch = 107; Training loss = 0.0078; Validation loss = 0.0068\n",
      "Epoch = 108; Training loss = 0.0078; Validation loss = 0.0076\n",
      "Epoch = 109; Training loss = 0.0077; Validation loss = 0.0074\n",
      "Epoch = 110; Training loss = 0.0071; Validation loss = 0.0068\n",
      "Epoch = 111; Training loss = 0.0070; Validation loss = 0.0064\n",
      "Epoch = 112; Training loss = 0.0072; Validation loss = 0.0064\n",
      "Epoch = 113; Training loss = 0.0067; Validation loss = 0.0065\n",
      "Epoch = 114; Training loss = 0.0070; Validation loss = 0.0072\n",
      "Epoch = 115; Training loss = 0.0071; Validation loss = 0.0073\n",
      "Epoch = 116; Training loss = 0.0076; Validation loss = 0.0071\n",
      "Epoch = 117; Training loss = 0.0073; Validation loss = 0.0076\n",
      "Epoch = 118; Training loss = 0.0064; Validation loss = 0.0065\n",
      "Epoch = 119; Training loss = 0.0068; Validation loss = 0.0065\n",
      "Epoch = 120; Training loss = 0.0078; Validation loss = 0.0068\n",
      "Epoch = 121; Training loss = 0.0068; Validation loss = 0.0066\n",
      "Epoch = 122; Training loss = 0.0059; Validation loss = 0.0072\n",
      "Epoch = 123; Training loss = 0.0065; Validation loss = 0.0069\n",
      "Epoch = 124; Training loss = 0.0062; Validation loss = 0.0072\n",
      "Epoch = 125; Training loss = 0.0064; Validation loss = 0.0073\n",
      "Epoch = 126; Training loss = 0.0067; Validation loss = 0.0068\n",
      "Epoch = 127; Training loss = 0.0062; Validation loss = 0.0073\n",
      "Epoch = 128; Training loss = 0.0061; Validation loss = 0.0068\n",
      "Epoch = 129; Training loss = 0.0061; Validation loss = 0.0077\n",
      "Epoch = 130; Training loss = 0.0066; Validation loss = 0.0076\n",
      "Epoch = 131; Training loss = 0.0059; Validation loss = 0.0079\n",
      "Validation loss too high; halting to prevent overfitting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 5839 of 5846. Accuracy: 99.88026000684228%\n",
      "Confusion Matrix:\n",
      "[[ 984    0    0    0    1    1]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    1    0 1012    0    0]\n",
      " [   1    0    0    1  972    1]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.001, 50) // Accuracy: 99.88026000684228%\n",
      "Epoch =   0; Training loss = 1.6196; Validation loss = 1.2606\n",
      "Epoch =   1; Training loss = 0.9556; Validation loss = 0.6029\n",
      "Epoch =   2; Training loss = 0.6197; Validation loss = 0.4060\n",
      "Epoch =   3; Training loss = 0.4835; Validation loss = 0.3193\n",
      "Epoch =   4; Training loss = 0.4083; Validation loss = 0.2726\n",
      "Epoch =   5; Training loss = 0.3558; Validation loss = 0.2380\n",
      "Epoch =   6; Training loss = 0.3232; Validation loss = 0.2165\n",
      "Epoch =   7; Training loss = 0.2956; Validation loss = 0.1987\n",
      "Epoch =   8; Training loss = 0.2721; Validation loss = 0.1839\n",
      "Epoch =   9; Training loss = 0.2525; Validation loss = 0.1707\n",
      "Epoch =  10; Training loss = 0.2368; Validation loss = 0.1584\n",
      "Epoch =  11; Training loss = 0.2211; Validation loss = 0.1483\n",
      "Epoch =  12; Training loss = 0.2108; Validation loss = 0.1401\n",
      "Epoch =  13; Training loss = 0.2020; Validation loss = 0.1339\n",
      "Epoch =  14; Training loss = 0.1904; Validation loss = 0.1281\n",
      "Epoch =  15; Training loss = 0.1828; Validation loss = 0.1208\n",
      "Epoch =  16; Training loss = 0.1747; Validation loss = 0.1175\n",
      "Epoch =  17; Training loss = 0.1665; Validation loss = 0.1117\n",
      "Epoch =  18; Training loss = 0.1595; Validation loss = 0.1074\n",
      "Epoch =  19; Training loss = 0.1551; Validation loss = 0.1026\n",
      "Epoch =  20; Training loss = 0.1489; Validation loss = 0.0978\n",
      "Epoch =  21; Training loss = 0.1412; Validation loss = 0.0959\n",
      "Epoch =  22; Training loss = 0.1390; Validation loss = 0.0906\n",
      "Epoch =  23; Training loss = 0.1330; Validation loss = 0.0870\n",
      "Epoch =  24; Training loss = 0.1295; Validation loss = 0.0857\n",
      "Epoch =  25; Training loss = 0.1282; Validation loss = 0.0843\n",
      "Epoch =  26; Training loss = 0.1226; Validation loss = 0.0790\n",
      "Epoch =  27; Training loss = 0.1201; Validation loss = 0.0765\n",
      "Epoch =  28; Training loss = 0.1175; Validation loss = 0.0744\n",
      "Epoch =  29; Training loss = 0.1115; Validation loss = 0.0726\n",
      "Epoch =  30; Training loss = 0.1076; Validation loss = 0.0701\n",
      "Epoch =  31; Training loss = 0.1072; Validation loss = 0.0688\n",
      "Epoch =  32; Training loss = 0.1054; Validation loss = 0.0617\n",
      "Epoch =  33; Training loss = 0.1049; Validation loss = 0.0630\n",
      "Epoch =  34; Training loss = 0.1001; Validation loss = 0.0609\n",
      "Epoch =  35; Training loss = 0.0962; Validation loss = 0.0618\n",
      "Epoch =  36; Training loss = 0.0938; Validation loss = 0.0588\n",
      "Epoch =  37; Training loss = 0.0914; Validation loss = 0.0579\n",
      "Epoch =  38; Training loss = 0.0881; Validation loss = 0.0569\n",
      "Epoch =  39; Training loss = 0.0869; Validation loss = 0.0556\n",
      "Epoch =  40; Training loss = 0.0851; Validation loss = 0.0527\n",
      "Epoch =  41; Training loss = 0.0838; Validation loss = 0.0555\n",
      "Epoch =  42; Training loss = 0.0810; Validation loss = 0.0525\n",
      "Epoch =  43; Training loss = 0.0799; Validation loss = 0.0503\n",
      "Epoch =  44; Training loss = 0.0763; Validation loss = 0.0509\n",
      "Epoch =  45; Training loss = 0.0757; Validation loss = 0.0485\n",
      "Epoch =  46; Training loss = 0.0775; Validation loss = 0.0488\n",
      "Epoch =  47; Training loss = 0.0729; Validation loss = 0.0470\n",
      "Epoch =  48; Training loss = 0.0714; Validation loss = 0.0457\n",
      "Epoch =  49; Training loss = 0.0705; Validation loss = 0.0469\n",
      "Epoch =  50; Training loss = 0.0686; Validation loss = 0.0479\n",
      "Epoch =  51; Training loss = 0.0690; Validation loss = 0.0423\n",
      "Epoch =  52; Training loss = 0.0657; Validation loss = 0.0439\n",
      "Epoch =  53; Training loss = 0.0650; Validation loss = 0.0408\n",
      "Epoch =  54; Training loss = 0.0633; Validation loss = 0.0414\n",
      "Epoch =  55; Training loss = 0.0631; Validation loss = 0.0402\n",
      "Epoch =  56; Training loss = 0.0631; Validation loss = 0.0395\n",
      "Epoch =  57; Training loss = 0.0608; Validation loss = 0.0368\n",
      "Epoch =  58; Training loss = 0.0608; Validation loss = 0.0376\n",
      "Epoch =  59; Training loss = 0.0586; Validation loss = 0.0397\n",
      "Epoch =  60; Training loss = 0.0583; Validation loss = 0.0381\n",
      "Epoch =  61; Training loss = 0.0596; Validation loss = 0.0382\n",
      "Epoch =  62; Training loss = 0.0554; Validation loss = 0.0358\n",
      "Epoch =  63; Training loss = 0.0561; Validation loss = 0.0361\n",
      "Epoch =  64; Training loss = 0.0565; Validation loss = 0.0348\n",
      "Epoch =  65; Training loss = 0.0524; Validation loss = 0.0362\n",
      "Epoch =  66; Training loss = 0.0522; Validation loss = 0.0344\n",
      "Epoch =  67; Training loss = 0.0512; Validation loss = 0.0327\n",
      "Epoch =  68; Training loss = 0.0526; Validation loss = 0.0343\n",
      "Epoch =  69; Training loss = 0.0521; Validation loss = 0.0390\n",
      "Epoch =  70; Training loss = 0.0504; Validation loss = 0.0316\n",
      "Epoch =  71; Training loss = 0.0502; Validation loss = 0.0322\n",
      "Epoch =  72; Training loss = 0.0493; Validation loss = 0.0323\n",
      "Epoch =  73; Training loss = 0.0487; Validation loss = 0.0346\n",
      "Epoch =  74; Training loss = 0.0464; Validation loss = 0.0307\n",
      "Epoch =  75; Training loss = 0.0475; Validation loss = 0.0300\n",
      "Epoch =  76; Training loss = 0.0467; Validation loss = 0.0320\n",
      "Epoch =  77; Training loss = 0.0460; Validation loss = 0.0301\n",
      "Epoch =  78; Training loss = 0.0437; Validation loss = 0.0292\n",
      "Epoch =  79; Training loss = 0.0430; Validation loss = 0.0313\n",
      "Epoch =  80; Training loss = 0.0451; Validation loss = 0.0296\n",
      "Epoch =  81; Training loss = 0.0441; Validation loss = 0.0292\n",
      "Epoch =  82; Training loss = 0.0423; Validation loss = 0.0318\n",
      "Epoch =  83; Training loss = 0.0420; Validation loss = 0.0289\n",
      "Epoch =  84; Training loss = 0.0407; Validation loss = 0.0307\n",
      "Epoch =  85; Training loss = 0.0433; Validation loss = 0.0302\n",
      "Epoch =  86; Training loss = 0.0419; Validation loss = 0.0283\n",
      "Epoch =  87; Training loss = 0.0422; Validation loss = 0.0289\n",
      "Epoch =  88; Training loss = 0.0419; Validation loss = 0.0281\n",
      "Epoch =  89; Training loss = 0.0393; Validation loss = 0.0260\n",
      "Epoch =  90; Training loss = 0.0385; Validation loss = 0.0319\n",
      "Epoch =  91; Training loss = 0.0404; Validation loss = 0.0273\n",
      "Epoch =  92; Training loss = 0.0390; Validation loss = 0.0291\n",
      "Epoch =  93; Training loss = 0.0363; Validation loss = 0.0273\n",
      "Epoch =  94; Training loss = 0.0386; Validation loss = 0.0266\n",
      "Epoch =  95; Training loss = 0.0374; Validation loss = 0.0272\n",
      "Epoch =  96; Training loss = 0.0375; Validation loss = 0.0262\n",
      "Epoch =  97; Training loss = 0.0374; Validation loss = 0.0307\n",
      "Epoch =  98; Training loss = 0.0361; Validation loss = 0.0287\n",
      "Epoch =  99; Training loss = 0.0356; Validation loss = 0.0252\n",
      "Epoch = 100; Training loss = 0.0357; Validation loss = 0.0266\n",
      "Epoch = 101; Training loss = 0.0359; Validation loss = 0.0259\n",
      "Epoch = 102; Training loss = 0.0344; Validation loss = 0.0264\n",
      "Epoch = 103; Training loss = 0.0348; Validation loss = 0.0265\n",
      "Epoch = 104; Training loss = 0.0329; Validation loss = 0.0270\n",
      "Epoch = 105; Training loss = 0.0325; Validation loss = 0.0271\n",
      "Epoch = 106; Training loss = 0.0328; Validation loss = 0.0264\n",
      "Epoch = 107; Training loss = 0.0337; Validation loss = 0.0253\n",
      "Epoch = 108; Training loss = 0.0318; Validation loss = 0.0268\n",
      "Epoch = 109; Training loss = 0.0326; Validation loss = 0.0248\n",
      "Epoch = 110; Training loss = 0.0349; Validation loss = 0.0257\n",
      "Epoch = 111; Training loss = 0.0311; Validation loss = 0.0254\n",
      "Epoch = 112; Training loss = 0.0319; Validation loss = 0.0266\n",
      "Epoch = 113; Training loss = 0.0303; Validation loss = 0.0259\n",
      "Epoch = 114; Training loss = 0.0287; Validation loss = 0.0255\n",
      "Epoch = 115; Training loss = 0.0320; Validation loss = 0.0263\n",
      "Epoch = 116; Training loss = 0.0301; Validation loss = 0.0253\n",
      "Epoch = 117; Training loss = 0.0304; Validation loss = 0.0243\n",
      "Epoch = 118; Training loss = 0.0308; Validation loss = 0.0261\n",
      "Epoch = 119; Training loss = 0.0295; Validation loss = 0.0242\n",
      "Epoch = 120; Training loss = 0.0297; Validation loss = 0.0244\n",
      "Epoch = 121; Training loss = 0.0314; Validation loss = 0.0237\n",
      "Epoch = 122; Training loss = 0.0299; Validation loss = 0.0240\n",
      "Epoch = 123; Training loss = 0.0287; Validation loss = 0.0242\n",
      "Epoch = 124; Training loss = 0.0295; Validation loss = 0.0247\n",
      "Epoch = 125; Training loss = 0.0292; Validation loss = 0.0237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 126; Training loss = 0.0283; Validation loss = 0.0240\n",
      "Epoch = 127; Training loss = 0.0272; Validation loss = 0.0261\n",
      "Epoch = 128; Training loss = 0.0284; Validation loss = 0.0259\n",
      "Epoch = 129; Training loss = 0.0279; Validation loss = 0.0235\n",
      "Epoch = 130; Training loss = 0.0280; Validation loss = 0.0254\n",
      "Epoch = 131; Training loss = 0.0272; Validation loss = 0.0234\n",
      "Epoch = 132; Training loss = 0.0269; Validation loss = 0.0214\n",
      "Epoch = 133; Training loss = 0.0269; Validation loss = 0.0230\n",
      "Epoch = 134; Training loss = 0.0269; Validation loss = 0.0224\n",
      "Epoch = 135; Training loss = 0.0273; Validation loss = 0.0258\n",
      "Epoch = 136; Training loss = 0.0272; Validation loss = 0.0241\n",
      "Epoch = 137; Training loss = 0.0268; Validation loss = 0.0241\n",
      "Epoch = 138; Training loss = 0.0277; Validation loss = 0.0229\n",
      "Epoch = 139; Training loss = 0.0262; Validation loss = 0.0227\n",
      "Epoch = 140; Training loss = 0.0242; Validation loss = 0.0218\n",
      "Epoch = 141; Training loss = 0.0261; Validation loss = 0.0235\n",
      "Epoch = 142; Training loss = 0.0239; Validation loss = 0.0231\n",
      "Epoch = 143; Training loss = 0.0255; Validation loss = 0.0229\n",
      "Epoch = 144; Training loss = 0.0250; Validation loss = 0.0240\n",
      "Epoch = 145; Training loss = 0.0256; Validation loss = 0.0247\n",
      "Epoch = 146; Training loss = 0.0239; Validation loss = 0.0228\n",
      "Epoch = 147; Training loss = 0.0262; Validation loss = 0.0252\n",
      "Epoch = 148; Training loss = 0.0244; Validation loss = 0.0155\n",
      "Epoch = 149; Training loss = 0.0254; Validation loss = 0.0231\n",
      "Epoch = 150; Training loss = 0.0235; Validation loss = 0.0228\n",
      "Epoch = 151; Training loss = 0.0226; Validation loss = 0.0236\n",
      "Epoch = 152; Training loss = 0.0236; Validation loss = 0.0216\n",
      "Epoch = 153; Training loss = 0.0240; Validation loss = 0.0221\n",
      "Epoch = 154; Training loss = 0.0225; Validation loss = 0.0239\n",
      "Epoch = 155; Training loss = 0.0239; Validation loss = 0.0215\n",
      "Epoch = 156; Training loss = 0.0233; Validation loss = 0.0222\n",
      "Epoch = 157; Training loss = 0.0236; Validation loss = 0.0223\n",
      "Epoch = 158; Training loss = 0.0223; Validation loss = 0.0232\n",
      "Epoch = 159; Training loss = 0.0237; Validation loss = 0.0218\n",
      "Epoch = 160; Training loss = 0.0216; Validation loss = 0.0231\n",
      "Epoch = 161; Training loss = 0.0220; Validation loss = 0.0220\n",
      "Epoch = 162; Training loss = 0.0227; Validation loss = 0.0214\n",
      "Epoch = 163; Training loss = 0.0211; Validation loss = 0.0224\n",
      "Epoch = 164; Training loss = 0.0214; Validation loss = 0.0222\n",
      "Epoch = 165; Training loss = 0.0214; Validation loss = 0.0218\n",
      "Epoch = 166; Training loss = 0.0212; Validation loss = 0.0238\n",
      "Epoch = 167; Training loss = 0.0229; Validation loss = 0.0223\n",
      "Epoch = 168; Training loss = 0.0210; Validation loss = 0.0218\n",
      "Epoch = 169; Training loss = 0.0204; Validation loss = 0.0221\n",
      "Epoch = 170; Training loss = 0.0206; Validation loss = 0.0246\n",
      "Epoch = 171; Training loss = 0.0204; Validation loss = 0.0231\n",
      "Epoch = 172; Training loss = 0.0204; Validation loss = 0.0222\n",
      "Epoch = 173; Training loss = 0.0233; Validation loss = 0.0220\n",
      "Epoch = 174; Training loss = 0.0204; Validation loss = 0.0224\n",
      "Epoch = 175; Training loss = 0.0192; Validation loss = 0.0225\n",
      "Epoch = 176; Training loss = 0.0196; Validation loss = 0.0232\n",
      "Epoch = 177; Training loss = 0.0222; Validation loss = 0.0220\n",
      "Epoch = 178; Training loss = 0.0208; Validation loss = 0.0217\n",
      "Epoch = 179; Training loss = 0.0192; Validation loss = 0.0226\n",
      "Epoch = 180; Training loss = 0.0206; Validation loss = 0.0235\n",
      "Epoch = 181; Training loss = 0.0193; Validation loss = 0.0224\n",
      "Epoch = 182; Training loss = 0.0217; Validation loss = 0.0225\n",
      "Epoch = 183; Training loss = 0.0198; Validation loss = 0.0217\n",
      "Epoch = 184; Training loss = 0.0191; Validation loss = 0.0247\n",
      "Epoch = 185; Training loss = 0.0199; Validation loss = 0.0225\n",
      "Epoch = 186; Training loss = 0.0195; Validation loss = 0.0221\n",
      "Epoch = 187; Training loss = 0.0204; Validation loss = 0.0217\n",
      "Epoch = 188; Training loss = 0.0196; Validation loss = 0.0223\n",
      "Epoch = 189; Training loss = 0.0187; Validation loss = 0.0159\n",
      "Epoch = 190; Training loss = 0.0202; Validation loss = 0.0311\n",
      "Epoch = 191; Training loss = 0.0198; Validation loss = 0.0220\n",
      "Epoch = 192; Training loss = 0.0202; Validation loss = 0.0212\n",
      "Epoch = 193; Training loss = 0.0183; Validation loss = 0.0201\n",
      "Epoch = 194; Training loss = 0.0175; Validation loss = 0.0217\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5838 of 5846. Accuracy: 99.86315429353404%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    0    1]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    1  979    1    0    0]\n",
      " [   0    0    0 1012    1    0]\n",
      " [   2    0    0    2  971    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.001, 100) // Accuracy: 99.86315429353404%\n",
      "Epoch =   0; Training loss = 1.7938; Validation loss = 1.7888\n",
      "Epoch =   1; Training loss = 1.7882; Validation loss = 1.7837\n",
      "Epoch =   2; Training loss = 1.7817; Validation loss = 1.7732\n",
      "Epoch =   3; Training loss = 1.7625; Validation loss = 1.7388\n",
      "Epoch =   4; Training loss = 1.6925; Validation loss = 1.6021\n",
      "Epoch =   5; Training loss = 1.4427; Validation loss = 1.1817\n",
      "Epoch =   6; Training loss = 1.0358; Validation loss = 0.7598\n",
      "Epoch =   7; Training loss = 0.7686; Validation loss = 0.5515\n",
      "Epoch =   8; Training loss = 0.6245; Validation loss = 0.4386\n",
      "Epoch =   9; Training loss = 0.5275; Validation loss = 0.3684\n",
      "Epoch =  10; Training loss = 0.4597; Validation loss = 0.3165\n",
      "Epoch =  11; Training loss = 0.4154; Validation loss = 0.2819\n",
      "Epoch =  12; Training loss = 0.3769; Validation loss = 0.2524\n",
      "Epoch =  13; Training loss = 0.3483; Validation loss = 0.2335\n",
      "Epoch =  14; Training loss = 0.3251; Validation loss = 0.2147\n",
      "Epoch =  15; Training loss = 0.3003; Validation loss = 0.2015\n",
      "Epoch =  16; Training loss = 0.2885; Validation loss = 0.1937\n",
      "Epoch =  17; Training loss = 0.2712; Validation loss = 0.1813\n",
      "Epoch =  18; Training loss = 0.2608; Validation loss = 0.1736\n",
      "Epoch =  19; Training loss = 0.2481; Validation loss = 0.1680\n",
      "Epoch =  20; Training loss = 0.2405; Validation loss = 0.1610\n",
      "Epoch =  21; Training loss = 0.2276; Validation loss = 0.1545\n",
      "Epoch =  22; Training loss = 0.2200; Validation loss = 0.1483\n",
      "Epoch =  23; Training loss = 0.2142; Validation loss = 0.1442\n",
      "Epoch =  24; Training loss = 0.2075; Validation loss = 0.1368\n",
      "Epoch =  25; Training loss = 0.1973; Validation loss = 0.1351\n",
      "Epoch =  26; Training loss = 0.1940; Validation loss = 0.1319\n",
      "Epoch =  27; Training loss = 0.1873; Validation loss = 0.1277\n",
      "Epoch =  28; Training loss = 0.1835; Validation loss = 0.1236\n",
      "Epoch =  29; Training loss = 0.1778; Validation loss = 0.1204\n",
      "Epoch =  30; Training loss = 0.1731; Validation loss = 0.1167\n",
      "Epoch =  31; Training loss = 0.1701; Validation loss = 0.1151\n",
      "Epoch =  32; Training loss = 0.1653; Validation loss = 0.1124\n",
      "Epoch =  33; Training loss = 0.1588; Validation loss = 0.1095\n",
      "Epoch =  34; Training loss = 0.1540; Validation loss = 0.1058\n",
      "Epoch =  35; Training loss = 0.1541; Validation loss = 0.1051\n",
      "Epoch =  36; Training loss = 0.1497; Validation loss = 0.1030\n",
      "Epoch =  37; Training loss = 0.1467; Validation loss = 0.1002\n",
      "Epoch =  38; Training loss = 0.1440; Validation loss = 0.0977\n",
      "Epoch =  39; Training loss = 0.1412; Validation loss = 0.0961\n",
      "Epoch =  40; Training loss = 0.1395; Validation loss = 0.0939\n",
      "Epoch =  41; Training loss = 0.1337; Validation loss = 0.0915\n",
      "Epoch =  42; Training loss = 0.1346; Validation loss = 0.0896\n",
      "Epoch =  43; Training loss = 0.1301; Validation loss = 0.0872\n",
      "Epoch =  44; Training loss = 0.1279; Validation loss = 0.0852\n",
      "Epoch =  45; Training loss = 0.1269; Validation loss = 0.0836\n",
      "Epoch =  46; Training loss = 0.1219; Validation loss = 0.0826\n",
      "Epoch =  47; Training loss = 0.1233; Validation loss = 0.0821\n",
      "Epoch =  48; Training loss = 0.1203; Validation loss = 0.0800\n",
      "Epoch =  49; Training loss = 0.1180; Validation loss = 0.0790\n",
      "Epoch =  50; Training loss = 0.1149; Validation loss = 0.0777\n",
      "Epoch =  51; Training loss = 0.1127; Validation loss = 0.0755\n",
      "Epoch =  52; Training loss = 0.1105; Validation loss = 0.0753\n",
      "Epoch =  53; Training loss = 0.1090; Validation loss = 0.0738\n",
      "Epoch =  54; Training loss = 0.1087; Validation loss = 0.0720\n",
      "Epoch =  55; Training loss = 0.1070; Validation loss = 0.0704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  56; Training loss = 0.1045; Validation loss = 0.0690\n",
      "Epoch =  57; Training loss = 0.1034; Validation loss = 0.0686\n",
      "Epoch =  58; Training loss = 0.1010; Validation loss = 0.0664\n",
      "Epoch =  59; Training loss = 0.0983; Validation loss = 0.0659\n",
      "Epoch =  60; Training loss = 0.0979; Validation loss = 0.0653\n",
      "Epoch =  61; Training loss = 0.0985; Validation loss = 0.0633\n",
      "Epoch =  62; Training loss = 0.0962; Validation loss = 0.0623\n",
      "Epoch =  63; Training loss = 0.0932; Validation loss = 0.0611\n",
      "Epoch =  64; Training loss = 0.0918; Validation loss = 0.0607\n",
      "Epoch =  65; Training loss = 0.0905; Validation loss = 0.0598\n",
      "Epoch =  66; Training loss = 0.0890; Validation loss = 0.0593\n",
      "Epoch =  67; Training loss = 0.0867; Validation loss = 0.0586\n",
      "Epoch =  68; Training loss = 0.0870; Validation loss = 0.0567\n",
      "Epoch =  69; Training loss = 0.0868; Validation loss = 0.0563\n",
      "Epoch =  70; Training loss = 0.0860; Validation loss = 0.0549\n",
      "Epoch =  71; Training loss = 0.0837; Validation loss = 0.0553\n",
      "Epoch =  72; Training loss = 0.0839; Validation loss = 0.0531\n",
      "Epoch =  73; Training loss = 0.0807; Validation loss = 0.0530\n",
      "Epoch =  74; Training loss = 0.0800; Validation loss = 0.0524\n",
      "Epoch =  75; Training loss = 0.0816; Validation loss = 0.0505\n",
      "Epoch =  76; Training loss = 0.0786; Validation loss = 0.0499\n",
      "Epoch =  77; Training loss = 0.0808; Validation loss = 0.0480\n",
      "Epoch =  78; Training loss = 0.0759; Validation loss = 0.0482\n",
      "Epoch =  79; Training loss = 0.0756; Validation loss = 0.0483\n",
      "Epoch =  80; Training loss = 0.0760; Validation loss = 0.0457\n",
      "Epoch =  81; Training loss = 0.0733; Validation loss = 0.0458\n",
      "Epoch =  82; Training loss = 0.0713; Validation loss = 0.0456\n",
      "Epoch =  83; Training loss = 0.0708; Validation loss = 0.0462\n",
      "Epoch =  84; Training loss = 0.0721; Validation loss = 0.0446\n",
      "Epoch =  85; Training loss = 0.0711; Validation loss = 0.0442\n",
      "Epoch =  86; Training loss = 0.0700; Validation loss = 0.0430\n",
      "Epoch =  87; Training loss = 0.0690; Validation loss = 0.0431\n",
      "Epoch =  88; Training loss = 0.0700; Validation loss = 0.0408\n",
      "Epoch =  89; Training loss = 0.0675; Validation loss = 0.0415\n",
      "Epoch =  90; Training loss = 0.0664; Validation loss = 0.0418\n",
      "Epoch =  91; Training loss = 0.0663; Validation loss = 0.0403\n",
      "Epoch =  92; Training loss = 0.0630; Validation loss = 0.0407\n",
      "Epoch =  93; Training loss = 0.0644; Validation loss = 0.0396\n",
      "Epoch =  94; Training loss = 0.0630; Validation loss = 0.0386\n",
      "Epoch =  95; Training loss = 0.0623; Validation loss = 0.0383\n",
      "Epoch =  96; Training loss = 0.0632; Validation loss = 0.0369\n",
      "Epoch =  97; Training loss = 0.0598; Validation loss = 0.0384\n",
      "Epoch =  98; Training loss = 0.0614; Validation loss = 0.0371\n",
      "Epoch =  99; Training loss = 0.0583; Validation loss = 0.0369\n",
      "Epoch = 100; Training loss = 0.0608; Validation loss = 0.0366\n",
      "Epoch = 101; Training loss = 0.0574; Validation loss = 0.0371\n",
      "Epoch = 102; Training loss = 0.0579; Validation loss = 0.0350\n",
      "Epoch = 103; Training loss = 0.0596; Validation loss = 0.0347\n",
      "Epoch = 104; Training loss = 0.0556; Validation loss = 0.0344\n",
      "Epoch = 105; Training loss = 0.0561; Validation loss = 0.0335\n",
      "Epoch = 106; Training loss = 0.0542; Validation loss = 0.0326\n",
      "Epoch = 107; Training loss = 0.0542; Validation loss = 0.0331\n",
      "Epoch = 108; Training loss = 0.0544; Validation loss = 0.0332\n",
      "Epoch = 109; Training loss = 0.0530; Validation loss = 0.0330\n",
      "Epoch = 110; Training loss = 0.0535; Validation loss = 0.0310\n",
      "Epoch = 111; Training loss = 0.0515; Validation loss = 0.0313\n",
      "Epoch = 112; Training loss = 0.0526; Validation loss = 0.0305\n",
      "Epoch = 113; Training loss = 0.0533; Validation loss = 0.0283\n",
      "Epoch = 114; Training loss = 0.0512; Validation loss = 0.0302\n",
      "Epoch = 115; Training loss = 0.0500; Validation loss = 0.0306\n",
      "Epoch = 116; Training loss = 0.0487; Validation loss = 0.0298\n",
      "Epoch = 117; Training loss = 0.0493; Validation loss = 0.0295\n",
      "Epoch = 118; Training loss = 0.0468; Validation loss = 0.0292\n",
      "Epoch = 119; Training loss = 0.0489; Validation loss = 0.0290\n",
      "Epoch = 120; Training loss = 0.0491; Validation loss = 0.0290\n",
      "Epoch = 121; Training loss = 0.0474; Validation loss = 0.0282\n",
      "Epoch = 122; Training loss = 0.0471; Validation loss = 0.0282\n",
      "Epoch = 123; Training loss = 0.0469; Validation loss = 0.0279\n",
      "Epoch = 124; Training loss = 0.0459; Validation loss = 0.0277\n",
      "Epoch = 125; Training loss = 0.0460; Validation loss = 0.0276\n",
      "Epoch = 126; Training loss = 0.0456; Validation loss = 0.0268\n",
      "Epoch = 127; Training loss = 0.0450; Validation loss = 0.0269\n",
      "Epoch = 128; Training loss = 0.0451; Validation loss = 0.0264\n",
      "Epoch = 129; Training loss = 0.0436; Validation loss = 0.0270\n",
      "Epoch = 130; Training loss = 0.0444; Validation loss = 0.0257\n",
      "Epoch = 131; Training loss = 0.0431; Validation loss = 0.0259\n",
      "Epoch = 132; Training loss = 0.0432; Validation loss = 0.0256\n",
      "Epoch = 133; Training loss = 0.0411; Validation loss = 0.0250\n",
      "Epoch = 134; Training loss = 0.0436; Validation loss = 0.0249\n",
      "Epoch = 135; Training loss = 0.0434; Validation loss = 0.0254\n",
      "Epoch = 136; Training loss = 0.0410; Validation loss = 0.0246\n",
      "Epoch = 137; Training loss = 0.0414; Validation loss = 0.0243\n",
      "Epoch = 138; Training loss = 0.0407; Validation loss = 0.0244\n",
      "Epoch = 139; Training loss = 0.0398; Validation loss = 0.0239\n",
      "Epoch = 140; Training loss = 0.0398; Validation loss = 0.0247\n",
      "Epoch = 141; Training loss = 0.0403; Validation loss = 0.0242\n",
      "Epoch = 142; Training loss = 0.0396; Validation loss = 0.0228\n",
      "Epoch = 143; Training loss = 0.0387; Validation loss = 0.0228\n",
      "Epoch = 144; Training loss = 0.0397; Validation loss = 0.0233\n",
      "Epoch = 145; Training loss = 0.0383; Validation loss = 0.0235\n",
      "Epoch = 146; Training loss = 0.0368; Validation loss = 0.0229\n",
      "Epoch = 147; Training loss = 0.0387; Validation loss = 0.0225\n",
      "Epoch = 148; Training loss = 0.0399; Validation loss = 0.0217\n",
      "Epoch = 149; Training loss = 0.0382; Validation loss = 0.0223\n",
      "Epoch = 150; Training loss = 0.0382; Validation loss = 0.0223\n",
      "Epoch = 151; Training loss = 0.0353; Validation loss = 0.0225\n",
      "Epoch = 152; Training loss = 0.0361; Validation loss = 0.0220\n",
      "Epoch = 153; Training loss = 0.0367; Validation loss = 0.0211\n",
      "Epoch = 154; Training loss = 0.0355; Validation loss = 0.0216\n",
      "Epoch = 155; Training loss = 0.0349; Validation loss = 0.0218\n",
      "Epoch = 156; Training loss = 0.0343; Validation loss = 0.0210\n",
      "Epoch = 157; Training loss = 0.0344; Validation loss = 0.0207\n",
      "Epoch = 158; Training loss = 0.0333; Validation loss = 0.0211\n",
      "Epoch = 159; Training loss = 0.0336; Validation loss = 0.0209\n",
      "Epoch = 160; Training loss = 0.0327; Validation loss = 0.0210\n",
      "Epoch = 161; Training loss = 0.0348; Validation loss = 0.0207\n",
      "Epoch = 162; Training loss = 0.0336; Validation loss = 0.0207\n",
      "Epoch = 163; Training loss = 0.0334; Validation loss = 0.0206\n",
      "Epoch = 164; Training loss = 0.0323; Validation loss = 0.0201\n",
      "Epoch = 165; Training loss = 0.0325; Validation loss = 0.0201\n",
      "Epoch = 166; Training loss = 0.0317; Validation loss = 0.0198\n",
      "Epoch = 167; Training loss = 0.0328; Validation loss = 0.0193\n",
      "Epoch = 168; Training loss = 0.0335; Validation loss = 0.0199\n",
      "Epoch = 169; Training loss = 0.0325; Validation loss = 0.0191\n",
      "Epoch = 170; Training loss = 0.0309; Validation loss = 0.0195\n",
      "Epoch = 171; Training loss = 0.0316; Validation loss = 0.0190\n",
      "Epoch = 172; Training loss = 0.0310; Validation loss = 0.0189\n",
      "Epoch = 173; Training loss = 0.0319; Validation loss = 0.0185\n",
      "Epoch = 174; Training loss = 0.0311; Validation loss = 0.0184\n",
      "Epoch = 175; Training loss = 0.0297; Validation loss = 0.0183\n",
      "Epoch = 176; Training loss = 0.0294; Validation loss = 0.0184\n",
      "Epoch = 177; Training loss = 0.0301; Validation loss = 0.0183\n",
      "Epoch = 178; Training loss = 0.0303; Validation loss = 0.0182\n",
      "Epoch = 179; Training loss = 0.0304; Validation loss = 0.0178\n",
      "Epoch = 180; Training loss = 0.0308; Validation loss = 0.0181\n",
      "Epoch = 181; Training loss = 0.0284; Validation loss = 0.0181\n",
      "Epoch = 182; Training loss = 0.0308; Validation loss = 0.0176\n",
      "Epoch = 183; Training loss = 0.0289; Validation loss = 0.0186\n",
      "Epoch = 184; Training loss = 0.0290; Validation loss = 0.0175\n",
      "Epoch = 185; Training loss = 0.0286; Validation loss = 0.0177\n",
      "Epoch = 186; Training loss = 0.0278; Validation loss = 0.0173\n",
      "Epoch = 187; Training loss = 0.0292; Validation loss = 0.0173\n",
      "Epoch = 188; Training loss = 0.0272; Validation loss = 0.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 189; Training loss = 0.0275; Validation loss = 0.0172\n",
      "Epoch = 190; Training loss = 0.0272; Validation loss = 0.0173\n",
      "Epoch = 191; Training loss = 0.0276; Validation loss = 0.0173\n",
      "Epoch = 192; Training loss = 0.0276; Validation loss = 0.0172\n",
      "Epoch = 193; Training loss = 0.0271; Validation loss = 0.0170\n",
      "Epoch = 194; Training loss = 0.0270; Validation loss = 0.0171\n",
      "Epoch = 195; Training loss = 0.0263; Validation loss = 0.0171\n",
      "Epoch = 196; Training loss = 0.0261; Validation loss = 0.0173\n",
      "Epoch = 197; Training loss = 0.0260; Validation loss = 0.0163\n",
      "Epoch = 198; Training loss = 0.0266; Validation loss = 0.0162\n",
      "Epoch = 199; Training loss = 0.0260; Validation loss = 0.0165\n",
      "Correct predictions: 5835 of 5846. Accuracy: 99.8118371536093%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    0    1]\n",
      " [   0  958    1    0    0    0]\n",
      " [   0    2  979    0    0    0]\n",
      " [   0    2    0 1009    2    0]\n",
      " [   0    0    0    2  972    1]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.001, 200) // Accuracy: 99.8118371536093%\n",
      "Epoch =   0; Training loss = 1.7878; Validation loss = 1.7795\n",
      "Epoch =   1; Training loss = 1.7666; Validation loss = 1.7511\n",
      "Epoch =   2; Training loss = 1.7287; Validation loss = 1.6972\n",
      "Epoch =   3; Training loss = 1.6534; Validation loss = 1.5881\n",
      "Epoch =   4; Training loss = 1.5145; Validation loss = 1.3965\n",
      "Epoch =   5; Training loss = 1.3020; Validation loss = 1.1340\n",
      "Epoch =   6; Training loss = 1.0654; Validation loss = 0.8770\n",
      "Epoch =   7; Training loss = 0.8766; Validation loss = 0.6919\n",
      "Epoch =   8; Training loss = 0.7452; Validation loss = 0.5716\n",
      "Epoch =   9; Training loss = 0.6564; Validation loss = 0.4957\n",
      "Epoch =  10; Training loss = 0.5926; Validation loss = 0.4402\n",
      "Epoch =  11; Training loss = 0.5450; Validation loss = 0.3972\n",
      "Epoch =  12; Training loss = 0.5048; Validation loss = 0.3671\n",
      "Epoch =  13; Training loss = 0.4800; Validation loss = 0.3419\n",
      "Epoch =  14; Training loss = 0.4493; Validation loss = 0.3238\n",
      "Epoch =  15; Training loss = 0.4290; Validation loss = 0.3078\n",
      "Epoch =  16; Training loss = 0.4110; Validation loss = 0.2908\n",
      "Epoch =  17; Training loss = 0.3906; Validation loss = 0.2803\n",
      "Epoch =  18; Training loss = 0.3765; Validation loss = 0.2681\n",
      "Epoch =  19; Training loss = 0.3621; Validation loss = 0.2582\n",
      "Epoch =  20; Training loss = 0.3498; Validation loss = 0.2481\n",
      "Epoch =  21; Training loss = 0.3379; Validation loss = 0.2397\n",
      "Epoch =  22; Training loss = 0.3299; Validation loss = 0.2356\n",
      "Epoch =  23; Training loss = 0.3211; Validation loss = 0.2262\n",
      "Epoch =  24; Training loss = 0.3105; Validation loss = 0.2173\n",
      "Epoch =  25; Training loss = 0.3019; Validation loss = 0.2168\n",
      "Epoch =  26; Training loss = 0.2947; Validation loss = 0.2086\n",
      "Epoch =  27; Training loss = 0.2882; Validation loss = 0.2038\n",
      "Epoch =  28; Training loss = 0.2790; Validation loss = 0.1968\n",
      "Epoch =  29; Training loss = 0.2745; Validation loss = 0.1920\n",
      "Epoch =  30; Training loss = 0.2663; Validation loss = 0.1902\n",
      "Epoch =  31; Training loss = 0.2595; Validation loss = 0.1837\n",
      "Epoch =  32; Training loss = 0.2555; Validation loss = 0.1794\n",
      "Epoch =  33; Training loss = 0.2485; Validation loss = 0.1773\n",
      "Epoch =  34; Training loss = 0.2444; Validation loss = 0.1730\n",
      "Epoch =  35; Training loss = 0.2418; Validation loss = 0.1690\n",
      "Epoch =  36; Training loss = 0.2346; Validation loss = 0.1686\n",
      "Epoch =  37; Training loss = 0.2313; Validation loss = 0.1651\n",
      "Epoch =  38; Training loss = 0.2267; Validation loss = 0.1601\n",
      "Epoch =  39; Training loss = 0.2220; Validation loss = 0.1563\n",
      "Epoch =  40; Training loss = 0.2219; Validation loss = 0.1561\n",
      "Epoch =  41; Training loss = 0.2135; Validation loss = 0.1520\n",
      "Epoch =  42; Training loss = 0.2109; Validation loss = 0.1488\n",
      "Epoch =  43; Training loss = 0.2088; Validation loss = 0.1479\n",
      "Epoch =  44; Training loss = 0.2071; Validation loss = 0.1429\n",
      "Epoch =  45; Training loss = 0.2012; Validation loss = 0.1412\n",
      "Epoch =  46; Training loss = 0.1979; Validation loss = 0.1399\n",
      "Epoch =  47; Training loss = 0.1947; Validation loss = 0.1373\n",
      "Epoch =  48; Training loss = 0.1925; Validation loss = 0.1344\n",
      "Epoch =  49; Training loss = 0.1902; Validation loss = 0.1338\n",
      "Epoch =  50; Training loss = 0.1864; Validation loss = 0.1323\n",
      "Epoch =  51; Training loss = 0.1843; Validation loss = 0.1301\n",
      "Epoch =  52; Training loss = 0.1814; Validation loss = 0.1267\n",
      "Epoch =  53; Training loss = 0.1791; Validation loss = 0.1285\n",
      "Epoch =  54; Training loss = 0.1787; Validation loss = 0.1254\n",
      "Epoch =  55; Training loss = 0.1752; Validation loss = 0.1244\n",
      "Epoch =  56; Training loss = 0.1704; Validation loss = 0.1203\n",
      "Epoch =  57; Training loss = 0.1710; Validation loss = 0.1200\n",
      "Epoch =  58; Training loss = 0.1674; Validation loss = 0.1190\n",
      "Epoch =  59; Training loss = 0.1668; Validation loss = 0.1175\n",
      "Epoch =  60; Training loss = 0.1657; Validation loss = 0.1165\n",
      "Epoch =  61; Training loss = 0.1649; Validation loss = 0.1149\n",
      "Epoch =  62; Training loss = 0.1610; Validation loss = 0.1132\n",
      "Epoch =  63; Training loss = 0.1577; Validation loss = 0.1111\n",
      "Epoch =  64; Training loss = 0.1585; Validation loss = 0.1113\n",
      "Epoch =  65; Training loss = 0.1557; Validation loss = 0.1102\n",
      "Epoch =  66; Training loss = 0.1544; Validation loss = 0.1079\n",
      "Epoch =  67; Training loss = 0.1510; Validation loss = 0.1079\n",
      "Epoch =  68; Training loss = 0.1509; Validation loss = 0.1081\n",
      "Epoch =  69; Training loss = 0.1460; Validation loss = 0.1053\n",
      "Epoch =  70; Training loss = 0.1470; Validation loss = 0.1040\n",
      "Epoch =  71; Training loss = 0.1469; Validation loss = 0.1032\n",
      "Epoch =  72; Training loss = 0.1435; Validation loss = 0.1020\n",
      "Epoch =  73; Training loss = 0.1430; Validation loss = 0.1021\n",
      "Epoch =  74; Training loss = 0.1425; Validation loss = 0.1009\n",
      "Epoch =  75; Training loss = 0.1405; Validation loss = 0.0998\n",
      "Epoch =  76; Training loss = 0.1395; Validation loss = 0.0986\n",
      "Epoch =  77; Training loss = 0.1372; Validation loss = 0.0963\n",
      "Epoch =  78; Training loss = 0.1357; Validation loss = 0.0959\n",
      "Epoch =  79; Training loss = 0.1337; Validation loss = 0.0929\n",
      "Epoch =  80; Training loss = 0.1342; Validation loss = 0.0952\n",
      "Epoch =  81; Training loss = 0.1335; Validation loss = 0.0948\n",
      "Epoch =  82; Training loss = 0.1337; Validation loss = 0.0946\n",
      "Epoch =  83; Training loss = 0.1319; Validation loss = 0.0911\n",
      "Epoch =  84; Training loss = 0.1310; Validation loss = 0.0924\n",
      "Epoch =  85; Training loss = 0.1270; Validation loss = 0.0887\n",
      "Epoch =  86; Training loss = 0.1263; Validation loss = 0.0897\n",
      "Epoch =  87; Training loss = 0.1277; Validation loss = 0.0889\n",
      "Epoch =  88; Training loss = 0.1218; Validation loss = 0.0881\n",
      "Epoch =  89; Training loss = 0.1229; Validation loss = 0.0859\n",
      "Epoch =  90; Training loss = 0.1232; Validation loss = 0.0844\n",
      "Epoch =  91; Training loss = 0.1198; Validation loss = 0.0853\n",
      "Epoch =  92; Training loss = 0.1211; Validation loss = 0.0849\n",
      "Epoch =  93; Training loss = 0.1198; Validation loss = 0.0837\n",
      "Epoch =  94; Training loss = 0.1201; Validation loss = 0.0843\n",
      "Epoch =  95; Training loss = 0.1157; Validation loss = 0.0834\n",
      "Epoch =  96; Training loss = 0.1178; Validation loss = 0.0796\n",
      "Epoch =  97; Training loss = 0.1148; Validation loss = 0.0797\n",
      "Epoch =  98; Training loss = 0.1144; Validation loss = 0.0804\n",
      "Epoch =  99; Training loss = 0.1134; Validation loss = 0.0796\n",
      "Epoch = 100; Training loss = 0.1120; Validation loss = 0.0796\n",
      "Epoch = 101; Training loss = 0.1119; Validation loss = 0.0793\n",
      "Epoch = 102; Training loss = 0.1125; Validation loss = 0.0775\n",
      "Epoch = 103; Training loss = 0.1115; Validation loss = 0.0780\n",
      "Epoch = 104; Training loss = 0.1085; Validation loss = 0.0771\n",
      "Epoch = 105; Training loss = 0.1081; Validation loss = 0.0711\n",
      "Epoch = 106; Training loss = 0.1060; Validation loss = 0.0756\n",
      "Epoch = 107; Training loss = 0.1063; Validation loss = 0.0749\n",
      "Epoch = 108; Training loss = 0.1065; Validation loss = 0.0734\n",
      "Epoch = 109; Training loss = 0.1048; Validation loss = 0.0735\n",
      "Epoch = 110; Training loss = 0.1034; Validation loss = 0.0719\n",
      "Epoch = 111; Training loss = 0.1013; Validation loss = 0.0718\n",
      "Epoch = 112; Training loss = 0.1024; Validation loss = 0.0687\n",
      "Epoch = 113; Training loss = 0.1027; Validation loss = 0.0703\n",
      "Epoch = 114; Training loss = 0.0994; Validation loss = 0.0686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 115; Training loss = 0.1006; Validation loss = 0.0711\n",
      "Epoch = 116; Training loss = 0.0994; Validation loss = 0.0702\n",
      "Epoch = 117; Training loss = 0.1001; Validation loss = 0.0699\n",
      "Epoch = 118; Training loss = 0.1013; Validation loss = 0.0691\n",
      "Epoch = 119; Training loss = 0.0982; Validation loss = 0.0661\n",
      "Epoch = 120; Training loss = 0.0961; Validation loss = 0.0678\n",
      "Epoch = 121; Training loss = 0.0945; Validation loss = 0.0676\n",
      "Epoch = 122; Training loss = 0.0968; Validation loss = 0.0666\n",
      "Epoch = 123; Training loss = 0.0965; Validation loss = 0.0671\n",
      "Epoch = 124; Training loss = 0.0956; Validation loss = 0.0656\n",
      "Epoch = 125; Training loss = 0.0933; Validation loss = 0.0640\n",
      "Epoch = 126; Training loss = 0.0941; Validation loss = 0.0627\n",
      "Epoch = 127; Training loss = 0.0935; Validation loss = 0.0637\n",
      "Epoch = 128; Training loss = 0.0916; Validation loss = 0.0642\n",
      "Epoch = 129; Training loss = 0.0915; Validation loss = 0.0632\n",
      "Epoch = 130; Training loss = 0.0925; Validation loss = 0.0603\n",
      "Epoch = 131; Training loss = 0.0878; Validation loss = 0.0609\n",
      "Epoch = 132; Training loss = 0.0895; Validation loss = 0.0622\n",
      "Epoch = 133; Training loss = 0.0878; Validation loss = 0.0595\n",
      "Epoch = 134; Training loss = 0.0883; Validation loss = 0.0595\n",
      "Epoch = 135; Training loss = 0.0859; Validation loss = 0.0595\n",
      "Epoch = 136; Training loss = 0.0864; Validation loss = 0.0586\n",
      "Epoch = 137; Training loss = 0.0876; Validation loss = 0.0607\n",
      "Epoch = 138; Training loss = 0.0842; Validation loss = 0.0546\n",
      "Epoch = 139; Training loss = 0.0840; Validation loss = 0.0543\n",
      "Epoch = 140; Training loss = 0.0843; Validation loss = 0.0577\n",
      "Epoch = 141; Training loss = 0.0840; Validation loss = 0.0559\n",
      "Epoch = 142; Training loss = 0.0822; Validation loss = 0.0588\n",
      "Epoch = 143; Training loss = 0.0842; Validation loss = 0.0560\n",
      "Epoch = 144; Training loss = 0.0831; Validation loss = 0.0565\n",
      "Epoch = 145; Training loss = 0.0820; Validation loss = 0.0557\n",
      "Epoch = 146; Training loss = 0.0824; Validation loss = 0.0561\n",
      "Epoch = 147; Training loss = 0.0810; Validation loss = 0.0559\n",
      "Epoch = 148; Training loss = 0.0801; Validation loss = 0.0554\n",
      "Epoch = 149; Training loss = 0.0796; Validation loss = 0.0542\n",
      "Epoch = 150; Training loss = 0.0801; Validation loss = 0.0553\n",
      "Epoch = 151; Training loss = 0.0784; Validation loss = 0.0545\n",
      "Epoch = 152; Training loss = 0.0789; Validation loss = 0.0540\n",
      "Epoch = 153; Training loss = 0.0784; Validation loss = 0.0495\n",
      "Epoch = 154; Training loss = 0.0763; Validation loss = 0.0530\n",
      "Epoch = 155; Training loss = 0.0770; Validation loss = 0.0515\n",
      "Epoch = 156; Training loss = 0.0758; Validation loss = 0.0491\n",
      "Epoch = 157; Training loss = 0.0774; Validation loss = 0.0519\n",
      "Epoch = 158; Training loss = 0.0776; Validation loss = 0.0519\n",
      "Epoch = 159; Training loss = 0.0767; Validation loss = 0.0464\n",
      "Epoch = 160; Training loss = 0.0738; Validation loss = 0.0504\n",
      "Epoch = 161; Training loss = 0.0752; Validation loss = 0.0505\n",
      "Epoch = 162; Training loss = 0.0738; Validation loss = 0.0510\n",
      "Epoch = 163; Training loss = 0.0737; Validation loss = 0.0492\n",
      "Epoch = 164; Training loss = 0.0748; Validation loss = 0.0497\n",
      "Epoch = 165; Training loss = 0.0720; Validation loss = 0.0516\n",
      "Epoch = 166; Training loss = 0.0727; Validation loss = 0.0476\n",
      "Epoch = 167; Training loss = 0.0721; Validation loss = 0.0473\n",
      "Epoch = 168; Training loss = 0.0710; Validation loss = 0.0490\n",
      "Epoch = 169; Training loss = 0.0726; Validation loss = 0.0462\n",
      "Epoch = 170; Training loss = 0.0708; Validation loss = 0.0478\n",
      "Epoch = 171; Training loss = 0.0711; Validation loss = 0.0479\n",
      "Epoch = 172; Training loss = 0.0707; Validation loss = 0.0463\n",
      "Epoch = 173; Training loss = 0.0698; Validation loss = 0.0472\n",
      "Epoch = 174; Training loss = 0.0688; Validation loss = 0.0470\n",
      "Epoch = 175; Training loss = 0.0695; Validation loss = 0.0471\n",
      "Epoch = 176; Training loss = 0.0684; Validation loss = 0.0468\n",
      "Epoch = 177; Training loss = 0.0684; Validation loss = 0.0466\n",
      "Epoch = 178; Training loss = 0.0691; Validation loss = 0.0473\n",
      "Epoch = 179; Training loss = 0.0687; Validation loss = 0.0464\n",
      "Epoch = 180; Training loss = 0.0664; Validation loss = 0.0441\n",
      "Epoch = 181; Training loss = 0.0661; Validation loss = 0.0444\n",
      "Epoch = 182; Training loss = 0.0671; Validation loss = 0.0448\n",
      "Epoch = 183; Training loss = 0.0661; Validation loss = 0.0443\n",
      "Epoch = 184; Training loss = 0.0669; Validation loss = 0.0453\n",
      "Epoch = 185; Training loss = 0.0664; Validation loss = 0.0448\n",
      "Epoch = 186; Training loss = 0.0652; Validation loss = 0.0434\n",
      "Epoch = 187; Training loss = 0.0657; Validation loss = 0.0426\n",
      "Epoch = 188; Training loss = 0.0666; Validation loss = 0.0425\n",
      "Epoch = 189; Training loss = 0.0644; Validation loss = 0.0424\n",
      "Epoch = 190; Training loss = 0.0631; Validation loss = 0.0428\n",
      "Epoch = 191; Training loss = 0.0645; Validation loss = 0.0425\n",
      "Epoch = 192; Training loss = 0.0639; Validation loss = 0.0422\n",
      "Epoch = 193; Training loss = 0.0629; Validation loss = 0.0428\n",
      "Epoch = 194; Training loss = 0.0611; Validation loss = 0.0419\n",
      "Epoch = 195; Training loss = 0.0621; Validation loss = 0.0354\n",
      "Epoch = 196; Training loss = 0.0603; Validation loss = 0.0422\n",
      "Epoch = 197; Training loss = 0.0624; Validation loss = 0.0411\n",
      "Epoch = 198; Training loss = 0.0612; Validation loss = 0.0411\n",
      "Epoch = 199; Training loss = 0.0619; Validation loss = 0.0396\n",
      "Correct predictions: 5797 of 5846. Accuracy: 99.161820047896%\n",
      "Confusion Matrix:\n",
      "[[983   1   0   0   1   1]\n",
      " [  0 958   1   0   0   0]\n",
      " [  0  13 967   1   0   0]\n",
      " [  0  16   3 994   0   0]\n",
      " [  2   0   1   5 965   2]\n",
      " [  0   2   0   0   0 930]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.001, 300) // Accuracy: 99.161820047896%\n",
      "Epoch =   0; Training loss = 1.7919; Validation loss = 1.7837\n",
      "Epoch =   1; Training loss = 1.7805; Validation loss = 1.7712\n",
      "Epoch =   2; Training loss = 1.7672; Validation loss = 1.7558\n",
      "Epoch =   3; Training loss = 1.7486; Validation loss = 1.7335\n",
      "Epoch =   4; Training loss = 1.7223; Validation loss = 1.7006\n",
      "Epoch =   5; Training loss = 1.6835; Validation loss = 1.6478\n",
      "Epoch =   6; Training loss = 1.6245; Validation loss = 1.5738\n",
      "Epoch =   7; Training loss = 1.5402; Validation loss = 1.4723\n",
      "Epoch =   8; Training loss = 1.4346; Validation loss = 1.3444\n",
      "Epoch =   9; Training loss = 1.3141; Validation loss = 1.2059\n",
      "Epoch =  10; Training loss = 1.1933; Validation loss = 1.0722\n",
      "Epoch =  11; Training loss = 1.0795; Validation loss = 0.9434\n",
      "Epoch =  12; Training loss = 0.9762; Validation loss = 0.8331\n",
      "Epoch =  13; Training loss = 0.8885; Validation loss = 0.7375\n",
      "Epoch =  14; Training loss = 0.8106; Validation loss = 0.6609\n",
      "Epoch =  15; Training loss = 0.7508; Validation loss = 0.5955\n",
      "Epoch =  16; Training loss = 0.6962; Validation loss = 0.5451\n",
      "Epoch =  17; Training loss = 0.6507; Validation loss = 0.5034\n",
      "Epoch =  18; Training loss = 0.6135; Validation loss = 0.4648\n",
      "Epoch =  19; Training loss = 0.5782; Validation loss = 0.4318\n",
      "Epoch =  20; Training loss = 0.5477; Validation loss = 0.4047\n",
      "Epoch =  21; Training loss = 0.5250; Validation loss = 0.3823\n",
      "Epoch =  22; Training loss = 0.5000; Validation loss = 0.3602\n",
      "Epoch =  23; Training loss = 0.4783; Validation loss = 0.3415\n",
      "Epoch =  24; Training loss = 0.4590; Validation loss = 0.3260\n",
      "Epoch =  25; Training loss = 0.4445; Validation loss = 0.3133\n",
      "Epoch =  26; Training loss = 0.4303; Validation loss = 0.3052\n",
      "Epoch =  27; Training loss = 0.4138; Validation loss = 0.2890\n",
      "Epoch =  28; Training loss = 0.4023; Validation loss = 0.2821\n",
      "Epoch =  29; Training loss = 0.3881; Validation loss = 0.2758\n",
      "Epoch =  30; Training loss = 0.3810; Validation loss = 0.2607\n",
      "Epoch =  31; Training loss = 0.3710; Validation loss = 0.2549\n",
      "Epoch =  32; Training loss = 0.3582; Validation loss = 0.2494\n",
      "Epoch =  33; Training loss = 0.3526; Validation loss = 0.2442\n",
      "Epoch =  34; Training loss = 0.3433; Validation loss = 0.2386\n",
      "Epoch =  35; Training loss = 0.3353; Validation loss = 0.2318\n",
      "Epoch =  36; Training loss = 0.3257; Validation loss = 0.2311\n",
      "Epoch =  37; Training loss = 0.3200; Validation loss = 0.2233\n",
      "Epoch =  38; Training loss = 0.3129; Validation loss = 0.2185\n",
      "Epoch =  39; Training loss = 0.3068; Validation loss = 0.2159\n",
      "Epoch =  40; Training loss = 0.2996; Validation loss = 0.2130\n",
      "Epoch =  41; Training loss = 0.2982; Validation loss = 0.2052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  42; Training loss = 0.2919; Validation loss = 0.2058\n",
      "Epoch =  43; Training loss = 0.2859; Validation loss = 0.1964\n",
      "Epoch =  44; Training loss = 0.2823; Validation loss = 0.1916\n",
      "Epoch =  45; Training loss = 0.2753; Validation loss = 0.1865\n",
      "Epoch =  46; Training loss = 0.2718; Validation loss = 0.1915\n",
      "Epoch =  47; Training loss = 0.2704; Validation loss = 0.1881\n",
      "Epoch =  48; Training loss = 0.2618; Validation loss = 0.1835\n",
      "Epoch =  49; Training loss = 0.2588; Validation loss = 0.1778\n",
      "Epoch =  50; Training loss = 0.2554; Validation loss = 0.1786\n",
      "Epoch =  51; Training loss = 0.2513; Validation loss = 0.1799\n",
      "Epoch =  52; Training loss = 0.2457; Validation loss = 0.1719\n",
      "Epoch =  53; Training loss = 0.2427; Validation loss = 0.1704\n",
      "Epoch =  54; Training loss = 0.2421; Validation loss = 0.1690\n",
      "Epoch =  55; Training loss = 0.2375; Validation loss = 0.1690\n",
      "Epoch =  56; Training loss = 0.2359; Validation loss = 0.1661\n",
      "Epoch =  57; Training loss = 0.2310; Validation loss = 0.1597\n",
      "Epoch =  58; Training loss = 0.2289; Validation loss = 0.1612\n",
      "Epoch =  59; Training loss = 0.2276; Validation loss = 0.1579\n",
      "Epoch =  60; Training loss = 0.2216; Validation loss = 0.1563\n",
      "Epoch =  61; Training loss = 0.2202; Validation loss = 0.1523\n",
      "Epoch =  62; Training loss = 0.2148; Validation loss = 0.1496\n",
      "Epoch =  63; Training loss = 0.2191; Validation loss = 0.1485\n",
      "Epoch =  64; Training loss = 0.2126; Validation loss = 0.1470\n",
      "Epoch =  65; Training loss = 0.2088; Validation loss = 0.1456\n",
      "Epoch =  66; Training loss = 0.2064; Validation loss = 0.1446\n",
      "Epoch =  67; Training loss = 0.2043; Validation loss = 0.1425\n",
      "Epoch =  68; Training loss = 0.1999; Validation loss = 0.1379\n",
      "Epoch =  69; Training loss = 0.2013; Validation loss = 0.1299\n",
      "Epoch =  70; Training loss = 0.1963; Validation loss = 0.1370\n",
      "Epoch =  71; Training loss = 0.1966; Validation loss = 0.1324\n",
      "Epoch =  72; Training loss = 0.1932; Validation loss = 0.1340\n",
      "Epoch =  73; Training loss = 0.1906; Validation loss = 0.1312\n",
      "Epoch =  74; Training loss = 0.1893; Validation loss = 0.1314\n",
      "Epoch =  75; Training loss = 0.1835; Validation loss = 0.1248\n",
      "Epoch =  76; Training loss = 0.1840; Validation loss = 0.1259\n",
      "Epoch =  77; Training loss = 0.1834; Validation loss = 0.1235\n",
      "Epoch =  78; Training loss = 0.1811; Validation loss = 0.1235\n",
      "Epoch =  79; Training loss = 0.1769; Validation loss = 0.1262\n",
      "Epoch =  80; Training loss = 0.1762; Validation loss = 0.1209\n",
      "Epoch =  81; Training loss = 0.1746; Validation loss = 0.1205\n",
      "Epoch =  82; Training loss = 0.1738; Validation loss = 0.1199\n",
      "Epoch =  83; Training loss = 0.1704; Validation loss = 0.1177\n",
      "Epoch =  84; Training loss = 0.1680; Validation loss = 0.1168\n",
      "Epoch =  85; Training loss = 0.1698; Validation loss = 0.1136\n",
      "Epoch =  86; Training loss = 0.1667; Validation loss = 0.1099\n",
      "Epoch =  87; Training loss = 0.1642; Validation loss = 0.1109\n",
      "Epoch =  88; Training loss = 0.1636; Validation loss = 0.1075\n",
      "Epoch =  89; Training loss = 0.1604; Validation loss = 0.1065\n",
      "Epoch =  90; Training loss = 0.1597; Validation loss = 0.1089\n",
      "Epoch =  91; Training loss = 0.1586; Validation loss = 0.1086\n",
      "Epoch =  92; Training loss = 0.1575; Validation loss = 0.1074\n",
      "Epoch =  93; Training loss = 0.1569; Validation loss = 0.1035\n",
      "Epoch =  94; Training loss = 0.1517; Validation loss = 0.1022\n",
      "Epoch =  95; Training loss = 0.1500; Validation loss = 0.1023\n",
      "Epoch =  96; Training loss = 0.1503; Validation loss = 0.1012\n",
      "Epoch =  97; Training loss = 0.1501; Validation loss = 0.0989\n",
      "Epoch =  98; Training loss = 0.1471; Validation loss = 0.1003\n",
      "Epoch =  99; Training loss = 0.1459; Validation loss = 0.0947\n",
      "Epoch = 100; Training loss = 0.1458; Validation loss = 0.0978\n",
      "Epoch = 101; Training loss = 0.1441; Validation loss = 0.0962\n",
      "Epoch = 102; Training loss = 0.1420; Validation loss = 0.0915\n",
      "Epoch = 103; Training loss = 0.1397; Validation loss = 0.0975\n",
      "Epoch = 104; Training loss = 0.1413; Validation loss = 0.0939\n",
      "Epoch = 105; Training loss = 0.1401; Validation loss = 0.0923\n",
      "Epoch = 106; Training loss = 0.1376; Validation loss = 0.0889\n",
      "Epoch = 107; Training loss = 0.1366; Validation loss = 0.0919\n",
      "Epoch = 108; Training loss = 0.1350; Validation loss = 0.0912\n",
      "Epoch = 109; Training loss = 0.1335; Validation loss = 0.0874\n",
      "Epoch = 110; Training loss = 0.1342; Validation loss = 0.0841\n",
      "Epoch = 111; Training loss = 0.1309; Validation loss = 0.0833\n",
      "Epoch = 112; Training loss = 0.1302; Validation loss = 0.0841\n",
      "Epoch = 113; Training loss = 0.1293; Validation loss = 0.0831\n",
      "Epoch = 114; Training loss = 0.1268; Validation loss = 0.0829\n",
      "Epoch = 115; Training loss = 0.1281; Validation loss = 0.0846\n",
      "Epoch = 116; Training loss = 0.1245; Validation loss = 0.0824\n",
      "Epoch = 117; Training loss = 0.1239; Validation loss = 0.0812\n",
      "Epoch = 118; Training loss = 0.1224; Validation loss = 0.0803\n",
      "Epoch = 119; Training loss = 0.1230; Validation loss = 0.0811\n",
      "Epoch = 120; Training loss = 0.1222; Validation loss = 0.0758\n",
      "Epoch = 121; Training loss = 0.1213; Validation loss = 0.0786\n",
      "Epoch = 122; Training loss = 0.1201; Validation loss = 0.0770\n",
      "Epoch = 123; Training loss = 0.1182; Validation loss = 0.0772\n",
      "Epoch = 124; Training loss = 0.1178; Validation loss = 0.0760\n",
      "Epoch = 125; Training loss = 0.1163; Validation loss = 0.0751\n",
      "Epoch = 126; Training loss = 0.1158; Validation loss = 0.0727\n",
      "Epoch = 127; Training loss = 0.1140; Validation loss = 0.0706\n",
      "Epoch = 128; Training loss = 0.1130; Validation loss = 0.0732\n",
      "Epoch = 129; Training loss = 0.1136; Validation loss = 0.0721\n",
      "Epoch = 130; Training loss = 0.1129; Validation loss = 0.0719\n",
      "Epoch = 131; Training loss = 0.1110; Validation loss = 0.0716\n",
      "Epoch = 132; Training loss = 0.1093; Validation loss = 0.0685\n",
      "Epoch = 133; Training loss = 0.1096; Validation loss = 0.0707\n",
      "Epoch = 134; Training loss = 0.1106; Validation loss = 0.0698\n",
      "Epoch = 135; Training loss = 0.1081; Validation loss = 0.0687\n",
      "Epoch = 136; Training loss = 0.1050; Validation loss = 0.0676\n",
      "Epoch = 137; Training loss = 0.1063; Validation loss = 0.0659\n",
      "Epoch = 138; Training loss = 0.1044; Validation loss = 0.0661\n",
      "Epoch = 139; Training loss = 0.1038; Validation loss = 0.0653\n",
      "Epoch = 140; Training loss = 0.1035; Validation loss = 0.0647\n",
      "Epoch = 141; Training loss = 0.1026; Validation loss = 0.0655\n",
      "Epoch = 142; Training loss = 0.1020; Validation loss = 0.0640\n",
      "Epoch = 143; Training loss = 0.1012; Validation loss = 0.0595\n",
      "Epoch = 144; Training loss = 0.0974; Validation loss = 0.0619\n",
      "Epoch = 145; Training loss = 0.1021; Validation loss = 0.0614\n",
      "Epoch = 146; Training loss = 0.0988; Validation loss = 0.0574\n",
      "Epoch = 147; Training loss = 0.0959; Validation loss = 0.0610\n",
      "Epoch = 148; Training loss = 0.0976; Validation loss = 0.0622\n",
      "Epoch = 149; Training loss = 0.0972; Validation loss = 0.0584\n",
      "Epoch = 150; Training loss = 0.0947; Validation loss = 0.0586\n",
      "Epoch = 151; Training loss = 0.0948; Validation loss = 0.0602\n",
      "Epoch = 152; Training loss = 0.0955; Validation loss = 0.0578\n",
      "Epoch = 153; Training loss = 0.0938; Validation loss = 0.0575\n",
      "Epoch = 154; Training loss = 0.0943; Validation loss = 0.0574\n",
      "Epoch = 155; Training loss = 0.0910; Validation loss = 0.0577\n",
      "Epoch = 156; Training loss = 0.0917; Validation loss = 0.0546\n",
      "Epoch = 157; Training loss = 0.0918; Validation loss = 0.0542\n",
      "Epoch = 158; Training loss = 0.0900; Validation loss = 0.0557\n",
      "Epoch = 159; Training loss = 0.0885; Validation loss = 0.0546\n",
      "Epoch = 160; Training loss = 0.0891; Validation loss = 0.0535\n",
      "Epoch = 161; Training loss = 0.0876; Validation loss = 0.0539\n",
      "Epoch = 162; Training loss = 0.0862; Validation loss = 0.0535\n",
      "Epoch = 163; Training loss = 0.0881; Validation loss = 0.0511\n",
      "Epoch = 164; Training loss = 0.0887; Validation loss = 0.0523\n",
      "Epoch = 165; Training loss = 0.0859; Validation loss = 0.0511\n",
      "Epoch = 166; Training loss = 0.0845; Validation loss = 0.0497\n",
      "Epoch = 167; Training loss = 0.0824; Validation loss = 0.0487\n",
      "Epoch = 168; Training loss = 0.0849; Validation loss = 0.0483\n",
      "Epoch = 169; Training loss = 0.0821; Validation loss = 0.0491\n",
      "Epoch = 170; Training loss = 0.0819; Validation loss = 0.0480\n",
      "Epoch = 171; Training loss = 0.0827; Validation loss = 0.0496\n",
      "Epoch = 172; Training loss = 0.0826; Validation loss = 0.0473\n",
      "Epoch = 173; Training loss = 0.0798; Validation loss = 0.0484\n",
      "Epoch = 174; Training loss = 0.0799; Validation loss = 0.0474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 175; Training loss = 0.0813; Validation loss = 0.0474\n",
      "Epoch = 176; Training loss = 0.0784; Validation loss = 0.0463\n",
      "Epoch = 177; Training loss = 0.0789; Validation loss = 0.0459\n",
      "Epoch = 178; Training loss = 0.0778; Validation loss = 0.0451\n",
      "Epoch = 179; Training loss = 0.0763; Validation loss = 0.0457\n",
      "Epoch = 180; Training loss = 0.0781; Validation loss = 0.0420\n",
      "Epoch = 181; Training loss = 0.0780; Validation loss = 0.0417\n",
      "Epoch = 182; Training loss = 0.0758; Validation loss = 0.0441\n",
      "Epoch = 183; Training loss = 0.0752; Validation loss = 0.0428\n",
      "Epoch = 184; Training loss = 0.0739; Validation loss = 0.0430\n",
      "Epoch = 185; Training loss = 0.0738; Validation loss = 0.0433\n",
      "Epoch = 186; Training loss = 0.0725; Validation loss = 0.0424\n",
      "Epoch = 187; Training loss = 0.0730; Validation loss = 0.0417\n",
      "Epoch = 188; Training loss = 0.0727; Validation loss = 0.0409\n",
      "Epoch = 189; Training loss = 0.0720; Validation loss = 0.0405\n",
      "Epoch = 190; Training loss = 0.0720; Validation loss = 0.0404\n",
      "Epoch = 191; Training loss = 0.0706; Validation loss = 0.0409\n",
      "Epoch = 192; Training loss = 0.0710; Validation loss = 0.0391\n",
      "Epoch = 193; Training loss = 0.0708; Validation loss = 0.0405\n",
      "Epoch = 194; Training loss = 0.0675; Validation loss = 0.0391\n",
      "Epoch = 195; Training loss = 0.0697; Validation loss = 0.0389\n",
      "Epoch = 196; Training loss = 0.0688; Validation loss = 0.0377\n",
      "Epoch = 197; Training loss = 0.0696; Validation loss = 0.0390\n",
      "Epoch = 198; Training loss = 0.0685; Validation loss = 0.0386\n",
      "Epoch = 199; Training loss = 0.0677; Validation loss = 0.0375\n",
      "Correct predictions: 5801 of 5846. Accuracy: 99.23024290112897%\n",
      "Confusion Matrix:\n",
      "[[985   0   0   0   0   1]\n",
      " [  0 957   2   0   0   0]\n",
      " [  0   8 972   1   0   0]\n",
      " [  0  17   3 993   0   0]\n",
      " [  3   0   0   5 966   1]\n",
      " [  0   3   1   0   0 928]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.001, 500) // Accuracy: 99.23024290112897%\n",
      "Epoch =   0; Training loss = 0.4678; Validation loss = 0.0849\n",
      "Epoch =   1; Training loss = 0.1045; Validation loss = 0.0431\n",
      "Epoch =   2; Training loss = 0.0681; Validation loss = 0.0270\n",
      "Epoch =   3; Training loss = 0.0502; Validation loss = 0.0197\n",
      "Epoch =   4; Training loss = 0.0424; Validation loss = 0.0160\n",
      "Epoch =   5; Training loss = 0.0351; Validation loss = 0.0139\n",
      "Epoch =   6; Training loss = 0.0332; Validation loss = 0.0132\n",
      "Epoch =   7; Training loss = 0.0271; Validation loss = 0.0130\n",
      "Epoch =   8; Training loss = 0.0232; Validation loss = 0.0138\n",
      "Epoch =   9; Training loss = 0.0211; Validation loss = 0.0119\n",
      "Epoch =  10; Training loss = 0.0199; Validation loss = 0.0090\n",
      "Epoch =  11; Training loss = 0.0188; Validation loss = 0.0102\n",
      "Epoch =  12; Training loss = 0.0174; Validation loss = 0.0103\n",
      "Epoch =  13; Training loss = 0.0162; Validation loss = 0.0101\n",
      "Epoch =  14; Training loss = 0.0150; Validation loss = 0.0079\n",
      "Epoch =  15; Training loss = 0.0145; Validation loss = 0.0091\n",
      "Epoch =  16; Training loss = 0.0126; Validation loss = 0.0094\n",
      "Epoch =  17; Training loss = 0.0155; Validation loss = 0.0086\n",
      "Epoch =  18; Training loss = 0.0123; Validation loss = 0.0116\n",
      "Epoch =  19; Training loss = 0.0126; Validation loss = 0.0089\n",
      "Epoch =  20; Training loss = 0.0111; Validation loss = 0.0103\n",
      "Epoch =  21; Training loss = 0.0091; Validation loss = 0.0115\n",
      "Epoch =  22; Training loss = 0.0113; Validation loss = 0.0086\n",
      "Epoch =  23; Training loss = 0.0110; Validation loss = 0.0107\n",
      "Epoch =  24; Training loss = 0.0091; Validation loss = 0.0101\n",
      "Epoch =  25; Training loss = 0.0082; Validation loss = 0.0092\n",
      "Epoch =  26; Training loss = 0.0086; Validation loss = 0.0149\n",
      "Epoch =  27; Training loss = 0.0080; Validation loss = 0.0127\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5839 of 5846. Accuracy: 99.88026000684228%\n",
      "Confusion Matrix:\n",
      "[[ 984    0    0    0    2    0]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   3    0    0    1  971    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.01, 50) // Accuracy: 99.88026000684228%\n",
      "Epoch =   0; Training loss = 0.6789; Validation loss = 0.1637\n",
      "Epoch =   1; Training loss = 0.1770; Validation loss = 0.0734\n",
      "Epoch =   2; Training loss = 0.1068; Validation loss = 0.0449\n",
      "Epoch =   3; Training loss = 0.0751; Validation loss = 0.0297\n",
      "Epoch =   4; Training loss = 0.0551; Validation loss = 0.0217\n",
      "Epoch =   5; Training loss = 0.0447; Validation loss = 0.0196\n",
      "Epoch =   6; Training loss = 0.0393; Validation loss = 0.0172\n",
      "Epoch =   7; Training loss = 0.0325; Validation loss = 0.0150\n",
      "Epoch =   8; Training loss = 0.0284; Validation loss = 0.0134\n",
      "Epoch =   9; Training loss = 0.0247; Validation loss = 0.0128\n",
      "Epoch =  10; Training loss = 0.0231; Validation loss = 0.0115\n",
      "Epoch =  11; Training loss = 0.0215; Validation loss = 0.0093\n",
      "Epoch =  12; Training loss = 0.0180; Validation loss = 0.0104\n",
      "Epoch =  13; Training loss = 0.0184; Validation loss = 0.0104\n",
      "Epoch =  14; Training loss = 0.0183; Validation loss = 0.0102\n",
      "Epoch =  15; Training loss = 0.0149; Validation loss = 0.0101\n",
      "Epoch =  16; Training loss = 0.0138; Validation loss = 0.0095\n",
      "Epoch =  17; Training loss = 0.0145; Validation loss = 0.0101\n",
      "Epoch =  18; Training loss = 0.0121; Validation loss = 0.0091\n",
      "Epoch =  19; Training loss = 0.0122; Validation loss = 0.0106\n",
      "Epoch =  20; Training loss = 0.0114; Validation loss = 0.0081\n",
      "Epoch =  21; Training loss = 0.0110; Validation loss = 0.0061\n",
      "Epoch =  22; Training loss = 0.0104; Validation loss = 0.0082\n",
      "Epoch =  23; Training loss = 0.0096; Validation loss = 0.0070\n",
      "Epoch =  24; Training loss = 0.0101; Validation loss = 0.0078\n",
      "Epoch =  25; Training loss = 0.0095; Validation loss = 0.0090\n",
      "Epoch =  26; Training loss = 0.0089; Validation loss = 0.0096\n",
      "Epoch =  27; Training loss = 0.0084; Validation loss = 0.0081\n",
      "Epoch =  28; Training loss = 0.0084; Validation loss = 0.0066\n",
      "Epoch =  29; Training loss = 0.0078; Validation loss = 0.0069\n",
      "Epoch =  30; Training loss = 0.0080; Validation loss = 0.0083\n",
      "Epoch =  31; Training loss = 0.0077; Validation loss = 0.0079\n",
      "Epoch =  32; Training loss = 0.0073; Validation loss = 0.0061\n",
      "Epoch =  33; Training loss = 0.0069; Validation loss = 0.0052\n",
      "Epoch =  34; Training loss = 0.0070; Validation loss = 0.0083\n",
      "Epoch =  35; Training loss = 0.0064; Validation loss = 0.0046\n",
      "Epoch =  36; Training loss = 0.0064; Validation loss = 0.0067\n",
      "Epoch =  37; Training loss = 0.0053; Validation loss = 0.0066\n",
      "Epoch =  38; Training loss = 0.0072; Validation loss = 0.0077\n",
      "Epoch =  39; Training loss = 0.0053; Validation loss = 0.0086\n",
      "Epoch =  40; Training loss = 0.0056; Validation loss = 0.0075\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5839 of 5846. Accuracy: 99.88026000684228%\n",
      "Confusion Matrix:\n",
      "[[ 982    0    0    0    4    0]\n",
      " [   0  958    1    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   0    0    0    1  974    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.01, 100) // Accuracy: 99.88026000684228%\n",
      "Epoch =   0; Training loss = 1.1080; Validation loss = 0.3682\n",
      "Epoch =   1; Training loss = 0.3698; Validation loss = 0.2146\n",
      "Epoch =   2; Training loss = 0.2534; Validation loss = 0.1566\n",
      "Epoch =   3; Training loss = 0.1996; Validation loss = 0.1157\n",
      "Epoch =   4; Training loss = 0.1625; Validation loss = 0.0972\n",
      "Epoch =   5; Training loss = 0.1383; Validation loss = 0.0782\n",
      "Epoch =   6; Training loss = 0.1167; Validation loss = 0.0643\n",
      "Epoch =   7; Training loss = 0.1021; Validation loss = 0.0603\n",
      "Epoch =   8; Training loss = 0.0907; Validation loss = 0.0497\n",
      "Epoch =   9; Training loss = 0.0813; Validation loss = 0.0423\n",
      "Epoch =  10; Training loss = 0.0722; Validation loss = 0.0369\n",
      "Epoch =  11; Training loss = 0.0641; Validation loss = 0.0387\n",
      "Epoch =  12; Training loss = 0.0582; Validation loss = 0.0655\n",
      "Epoch =  13; Training loss = 0.0549; Validation loss = 0.0284\n",
      "Epoch =  14; Training loss = 0.0501; Validation loss = 0.0253\n",
      "Epoch =  15; Training loss = 0.0443; Validation loss = 0.0221\n",
      "Epoch =  16; Training loss = 0.0403; Validation loss = 0.0200\n",
      "Epoch =  17; Training loss = 0.0398; Validation loss = 0.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  18; Training loss = 0.0363; Validation loss = 0.0195\n",
      "Epoch =  19; Training loss = 0.0339; Validation loss = 0.0167\n",
      "Epoch =  20; Training loss = 0.0307; Validation loss = 0.0172\n",
      "Epoch =  21; Training loss = 0.0310; Validation loss = 0.0178\n",
      "Epoch =  22; Training loss = 0.0277; Validation loss = 0.0144\n",
      "Epoch =  23; Training loss = 0.0247; Validation loss = 0.0151\n",
      "Epoch =  24; Training loss = 0.0254; Validation loss = 0.0151\n",
      "Epoch =  25; Training loss = 0.0236; Validation loss = 0.0137\n",
      "Epoch =  26; Training loss = 0.0247; Validation loss = 0.0141\n",
      "Epoch =  27; Training loss = 0.0215; Validation loss = 0.0129\n",
      "Epoch =  28; Training loss = 0.0220; Validation loss = 0.0155\n",
      "Epoch =  29; Training loss = 0.0208; Validation loss = 0.0158\n",
      "Epoch =  30; Training loss = 0.0197; Validation loss = 0.0118\n",
      "Epoch =  31; Training loss = 0.0188; Validation loss = 0.0137\n",
      "Epoch =  32; Training loss = 0.0187; Validation loss = 0.0128\n",
      "Epoch =  33; Training loss = 0.0176; Validation loss = 0.0121\n",
      "Epoch =  34; Training loss = 0.0162; Validation loss = 0.0123\n",
      "Epoch =  35; Training loss = 0.0157; Validation loss = 0.0106\n",
      "Epoch =  36; Training loss = 0.0155; Validation loss = 0.0126\n",
      "Epoch =  37; Training loss = 0.0165; Validation loss = 0.0138\n",
      "Epoch =  38; Training loss = 0.0147; Validation loss = 0.0115\n",
      "Epoch =  39; Training loss = 0.0139; Validation loss = 0.0106\n",
      "Epoch =  40; Training loss = 0.0139; Validation loss = 0.0092\n",
      "Epoch =  41; Training loss = 0.0138; Validation loss = 0.0131\n",
      "Epoch =  42; Training loss = 0.0134; Validation loss = 0.0105\n",
      "Epoch =  43; Training loss = 0.0134; Validation loss = 0.0114\n",
      "Epoch =  44; Training loss = 0.0130; Validation loss = 0.0113\n",
      "Epoch =  45; Training loss = 0.0130; Validation loss = 0.0102\n",
      "Epoch =  46; Training loss = 0.0128; Validation loss = 0.0114\n",
      "Epoch =  47; Training loss = 0.0120; Validation loss = 0.0114\n",
      "Epoch =  48; Training loss = 0.0109; Validation loss = 0.0118\n",
      "Epoch =  49; Training loss = 0.0109; Validation loss = 0.0115\n",
      "Epoch =  50; Training loss = 0.0106; Validation loss = 0.0105\n",
      "Epoch =  51; Training loss = 0.0109; Validation loss = 0.0122\n",
      "Epoch =  52; Training loss = 0.0104; Validation loss = 0.0107\n",
      "Epoch =  53; Training loss = 0.0106; Validation loss = 0.0111\n",
      "Epoch =  54; Training loss = 0.0099; Validation loss = 0.0099\n",
      "Epoch =  55; Training loss = 0.0096; Validation loss = 0.0117\n",
      "Epoch =  56; Training loss = 0.0101; Validation loss = 0.0103\n",
      "Epoch =  57; Training loss = 0.0094; Validation loss = 0.0099\n",
      "Epoch =  58; Training loss = 0.0098; Validation loss = 0.0100\n",
      "Epoch =  59; Training loss = 0.0081; Validation loss = 0.0111\n",
      "Epoch =  60; Training loss = 0.0096; Validation loss = 0.0115\n",
      "Epoch =  61; Training loss = 0.0090; Validation loss = 0.0101\n",
      "Epoch =  62; Training loss = 0.0081; Validation loss = 0.0107\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5842 of 5846. Accuracy: 99.93157714676703%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    1    0]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   1    0    0    1  973    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.01, 200) // Accuracy: 99.93157714676703%\n",
      "Epoch =   0; Training loss = 1.6045; Validation loss = 0.9318\n",
      "Epoch =   1; Training loss = 0.6434; Validation loss = 0.3262\n",
      "Epoch =   2; Training loss = 0.3741; Validation loss = 0.2215\n",
      "Epoch =   3; Training loss = 0.2851; Validation loss = 0.1697\n",
      "Epoch =   4; Training loss = 0.2370; Validation loss = 0.1367\n",
      "Epoch =   5; Training loss = 0.2006; Validation loss = 0.1146\n",
      "Epoch =   6; Training loss = 0.1744; Validation loss = 0.0996\n",
      "Epoch =   7; Training loss = 0.1545; Validation loss = 0.0929\n",
      "Epoch =   8; Training loss = 0.1389; Validation loss = 0.0788\n",
      "Epoch =   9; Training loss = 0.1281; Validation loss = 0.0702\n",
      "Epoch =  10; Training loss = 0.1151; Validation loss = 0.0610\n",
      "Epoch =  11; Training loss = 0.1062; Validation loss = 0.0584\n",
      "Epoch =  12; Training loss = 0.0981; Validation loss = 0.0498\n",
      "Epoch =  13; Training loss = 0.0917; Validation loss = 0.0473\n",
      "Epoch =  14; Training loss = 0.0872; Validation loss = 0.0448\n",
      "Epoch =  15; Training loss = 0.0779; Validation loss = 0.0413\n",
      "Epoch =  16; Training loss = 0.0755; Validation loss = 0.0401\n",
      "Epoch =  17; Training loss = 0.0708; Validation loss = 0.0396\n",
      "Epoch =  18; Training loss = 0.0649; Validation loss = 0.0353\n",
      "Epoch =  19; Training loss = 0.0644; Validation loss = 0.0329\n",
      "Epoch =  20; Training loss = 0.0618; Validation loss = 0.0374\n",
      "Epoch =  21; Training loss = 0.0572; Validation loss = 0.0381\n",
      "Epoch =  22; Training loss = 0.0539; Validation loss = 0.0295\n",
      "Epoch =  23; Training loss = 0.0535; Validation loss = 0.0275\n",
      "Epoch =  24; Training loss = 0.0512; Validation loss = 0.0321\n",
      "Epoch =  25; Training loss = 0.0470; Validation loss = 0.0247\n",
      "Epoch =  26; Training loss = 0.0467; Validation loss = 0.0248\n",
      "Epoch =  27; Training loss = 0.0439; Validation loss = 0.0242\n",
      "Epoch =  28; Training loss = 0.0424; Validation loss = 0.0230\n",
      "Epoch =  29; Training loss = 0.0398; Validation loss = 0.0240\n",
      "Epoch =  30; Training loss = 0.0392; Validation loss = 0.0225\n",
      "Epoch =  31; Training loss = 0.0383; Validation loss = 0.0227\n",
      "Epoch =  32; Training loss = 0.0380; Validation loss = 0.0199\n",
      "Epoch =  33; Training loss = 0.0352; Validation loss = 0.0208\n",
      "Epoch =  34; Training loss = 0.0345; Validation loss = 0.0204\n",
      "Epoch =  35; Training loss = 0.0337; Validation loss = 0.0204\n",
      "Epoch =  36; Training loss = 0.0332; Validation loss = 0.0196\n",
      "Epoch =  37; Training loss = 0.0330; Validation loss = 0.0201\n",
      "Epoch =  38; Training loss = 0.0314; Validation loss = 0.0190\n",
      "Epoch =  39; Training loss = 0.0307; Validation loss = 0.0175\n",
      "Epoch =  40; Training loss = 0.0303; Validation loss = 0.0182\n",
      "Epoch =  41; Training loss = 0.0284; Validation loss = 0.0184\n",
      "Epoch =  42; Training loss = 0.0288; Validation loss = 0.0183\n",
      "Epoch =  43; Training loss = 0.0264; Validation loss = 0.0178\n",
      "Epoch =  44; Training loss = 0.0276; Validation loss = 0.0189\n",
      "Epoch =  45; Training loss = 0.0241; Validation loss = 0.0177\n",
      "Epoch =  46; Training loss = 0.0254; Validation loss = 0.0149\n",
      "Epoch =  47; Training loss = 0.0248; Validation loss = 0.0200\n",
      "Epoch =  48; Training loss = 0.0244; Validation loss = 0.0178\n",
      "Epoch =  49; Training loss = 0.0229; Validation loss = 0.0167\n",
      "Epoch =  50; Training loss = 0.0222; Validation loss = 0.0170\n",
      "Epoch =  51; Training loss = 0.0226; Validation loss = 0.0175\n",
      "Epoch =  52; Training loss = 0.0227; Validation loss = 0.0149\n",
      "Epoch =  53; Training loss = 0.0216; Validation loss = 0.0158\n",
      "Epoch =  54; Training loss = 0.0223; Validation loss = 0.0154\n",
      "Epoch =  55; Training loss = 0.0220; Validation loss = 0.0154\n",
      "Epoch =  56; Training loss = 0.0200; Validation loss = 0.0151\n",
      "Epoch =  57; Training loss = 0.0195; Validation loss = 0.0140\n",
      "Epoch =  58; Training loss = 0.0200; Validation loss = 0.0157\n",
      "Epoch =  59; Training loss = 0.0184; Validation loss = 0.0131\n",
      "Epoch =  60; Training loss = 0.0193; Validation loss = 0.0114\n",
      "Epoch =  61; Training loss = 0.0194; Validation loss = 0.0163\n",
      "Epoch =  62; Training loss = 0.0182; Validation loss = 0.0153\n",
      "Epoch =  63; Training loss = 0.0179; Validation loss = 0.0143\n",
      "Epoch =  64; Training loss = 0.0174; Validation loss = 0.0145\n",
      "Epoch =  65; Training loss = 0.0172; Validation loss = 0.0156\n",
      "Epoch =  66; Training loss = 0.0170; Validation loss = 0.0144\n",
      "Epoch =  67; Training loss = 0.0175; Validation loss = 0.0162\n",
      "Epoch =  68; Training loss = 0.0164; Validation loss = 0.0159\n",
      "Epoch =  69; Training loss = 0.0167; Validation loss = 0.0134\n",
      "Epoch =  70; Training loss = 0.0157; Validation loss = 0.0149\n",
      "Epoch =  71; Training loss = 0.0170; Validation loss = 0.0175\n",
      "Epoch =  72; Training loss = 0.0150; Validation loss = 0.0141\n",
      "Epoch =  73; Training loss = 0.0164; Validation loss = 0.0132\n",
      "Epoch =  74; Training loss = 0.0141; Validation loss = 0.0158\n",
      "Epoch =  75; Training loss = 0.0146; Validation loss = 0.0153\n",
      "Epoch =  76; Training loss = 0.0152; Validation loss = 0.0133\n",
      "Epoch =  77; Training loss = 0.0136; Validation loss = 0.0159\n",
      "Epoch =  78; Training loss = 0.0146; Validation loss = 0.0134\n",
      "Epoch =  79; Training loss = 0.0136; Validation loss = 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  80; Training loss = 0.0156; Validation loss = 0.0141\n",
      "Epoch =  81; Training loss = 0.0135; Validation loss = 0.0131\n",
      "Epoch =  82; Training loss = 0.0132; Validation loss = 0.0141\n",
      "Epoch =  83; Training loss = 0.0127; Validation loss = 0.0139\n",
      "Epoch =  84; Training loss = 0.0129; Validation loss = 0.0126\n",
      "Epoch =  85; Training loss = 0.0130; Validation loss = 0.0132\n",
      "Epoch =  86; Training loss = 0.0126; Validation loss = 0.0159\n",
      "Epoch =  87; Training loss = 0.0144; Validation loss = 0.0127\n",
      "Epoch =  88; Training loss = 0.0118; Validation loss = 0.0149\n",
      "Epoch =  89; Training loss = 0.0121; Validation loss = 0.0137\n",
      "Epoch =  90; Training loss = 0.0126; Validation loss = 0.0133\n",
      "Epoch =  91; Training loss = 0.0123; Validation loss = 0.0136\n",
      "Epoch =  92; Training loss = 0.0121; Validation loss = 0.0139\n",
      "Epoch =  93; Training loss = 0.0111; Validation loss = 0.0134\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5837 of 5846. Accuracy: 99.8460485802258%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    0    1]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    2  979    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   3    0    1    2  969    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.01, 300) // Accuracy: 99.8460485802258%\n",
      "Epoch =   0; Training loss = 1.5485; Validation loss = 0.9749\n",
      "Epoch =   1; Training loss = 0.7212; Validation loss = 0.3972\n",
      "Epoch =   2; Training loss = 0.4428; Validation loss = 0.2699\n",
      "Epoch =   3; Training loss = 0.3425; Validation loss = 0.2133\n",
      "Epoch =   4; Training loss = 0.2823; Validation loss = 0.1750\n",
      "Epoch =   5; Training loss = 0.2450; Validation loss = 0.1538\n",
      "Epoch =   6; Training loss = 0.2158; Validation loss = 0.1308\n",
      "Epoch =   7; Training loss = 0.1951; Validation loss = 0.1238\n",
      "Epoch =   8; Training loss = 0.1786; Validation loss = 0.1122\n",
      "Epoch =   9; Training loss = 0.1661; Validation loss = 0.1037\n",
      "Epoch =  10; Training loss = 0.1577; Validation loss = 0.0988\n",
      "Epoch =  11; Training loss = 0.1462; Validation loss = 0.0919\n",
      "Epoch =  12; Training loss = 0.1359; Validation loss = 0.0876\n",
      "Epoch =  13; Training loss = 0.1283; Validation loss = 0.0824\n",
      "Epoch =  14; Training loss = 0.1218; Validation loss = 0.0783\n",
      "Epoch =  15; Training loss = 0.1156; Validation loss = 0.0739\n",
      "Epoch =  16; Training loss = 0.1129; Validation loss = 0.0716\n",
      "Epoch =  17; Training loss = 0.1072; Validation loss = 0.0637\n",
      "Epoch =  18; Training loss = 0.1025; Validation loss = 0.0643\n",
      "Epoch =  19; Training loss = 0.0976; Validation loss = 0.0622\n",
      "Epoch =  20; Training loss = 0.0957; Validation loss = 0.0573\n",
      "Epoch =  21; Training loss = 0.0906; Validation loss = 0.0558\n",
      "Epoch =  22; Training loss = 0.0867; Validation loss = 0.0536\n",
      "Epoch =  23; Training loss = 0.0853; Validation loss = 0.0542\n",
      "Epoch =  24; Training loss = 0.0808; Validation loss = 0.0483\n",
      "Epoch =  25; Training loss = 0.0792; Validation loss = 0.0495\n",
      "Epoch =  26; Training loss = 0.0761; Validation loss = 0.0453\n",
      "Epoch =  27; Training loss = 0.0746; Validation loss = 0.0387\n",
      "Epoch =  28; Training loss = 0.0719; Validation loss = 0.0426\n",
      "Epoch =  29; Training loss = 0.0675; Validation loss = 0.0406\n",
      "Epoch =  30; Training loss = 0.0668; Validation loss = 0.0388\n",
      "Epoch =  31; Training loss = 0.0635; Validation loss = 0.0378\n",
      "Epoch =  32; Training loss = 0.0633; Validation loss = 0.0370\n",
      "Epoch =  33; Training loss = 0.0616; Validation loss = 0.0304\n",
      "Epoch =  34; Training loss = 0.0580; Validation loss = 0.0326\n",
      "Epoch =  35; Training loss = 0.0568; Validation loss = 0.0319\n",
      "Epoch =  36; Training loss = 0.0551; Validation loss = 0.0321\n",
      "Epoch =  37; Training loss = 0.0548; Validation loss = 0.0310\n",
      "Epoch =  38; Training loss = 0.0523; Validation loss = 0.0296\n",
      "Epoch =  39; Training loss = 0.0508; Validation loss = 0.0293\n",
      "Epoch =  40; Training loss = 0.0487; Validation loss = 0.0288\n",
      "Epoch =  41; Training loss = 0.0496; Validation loss = 0.0282\n",
      "Epoch =  42; Training loss = 0.0464; Validation loss = 0.0277\n",
      "Epoch =  43; Training loss = 0.0468; Validation loss = 0.0272\n",
      "Epoch =  44; Training loss = 0.0444; Validation loss = 0.0253\n",
      "Epoch =  45; Training loss = 0.0431; Validation loss = 0.0254\n",
      "Epoch =  46; Training loss = 0.0416; Validation loss = 0.0249\n",
      "Epoch =  47; Training loss = 0.0419; Validation loss = 0.0230\n",
      "Epoch =  48; Training loss = 0.0403; Validation loss = 0.0234\n",
      "Epoch =  49; Training loss = 0.0403; Validation loss = 0.0234\n",
      "Epoch =  50; Training loss = 0.0386; Validation loss = 0.0217\n",
      "Epoch =  51; Training loss = 0.0366; Validation loss = 0.0197\n",
      "Epoch =  52; Training loss = 0.0375; Validation loss = 0.0206\n",
      "Epoch =  53; Training loss = 0.0359; Validation loss = 0.0212\n",
      "Epoch =  54; Training loss = 0.0349; Validation loss = 0.0213\n",
      "Epoch =  55; Training loss = 0.0335; Validation loss = 0.0200\n",
      "Epoch =  56; Training loss = 0.0334; Validation loss = 0.0158\n",
      "Epoch =  57; Training loss = 0.0326; Validation loss = 0.0190\n",
      "Epoch =  58; Training loss = 0.0320; Validation loss = 0.0199\n",
      "Epoch =  59; Training loss = 0.0314; Validation loss = 0.0170\n",
      "Epoch =  60; Training loss = 0.0318; Validation loss = 0.0200\n",
      "Epoch =  61; Training loss = 0.0295; Validation loss = 0.0192\n",
      "Epoch =  62; Training loss = 0.0317; Validation loss = 0.0175\n",
      "Epoch =  63; Training loss = 0.0318; Validation loss = 0.0184\n",
      "Epoch =  64; Training loss = 0.0287; Validation loss = 0.0188\n",
      "Epoch =  65; Training loss = 0.0285; Validation loss = 0.0172\n",
      "Epoch =  66; Training loss = 0.0272; Validation loss = 0.0183\n",
      "Epoch =  67; Training loss = 0.0279; Validation loss = 0.0171\n",
      "Epoch =  68; Training loss = 0.0273; Validation loss = 0.0151\n",
      "Epoch =  69; Training loss = 0.0270; Validation loss = 0.0169\n",
      "Epoch =  70; Training loss = 0.0267; Validation loss = 0.0166\n",
      "Epoch =  71; Training loss = 0.0257; Validation loss = 0.0123\n",
      "Epoch =  72; Training loss = 0.0243; Validation loss = 0.0154\n",
      "Epoch =  73; Training loss = 0.0258; Validation loss = 0.0169\n",
      "Epoch =  74; Training loss = 0.0249; Validation loss = 0.0107\n",
      "Epoch =  75; Training loss = 0.0239; Validation loss = 0.0156\n",
      "Epoch =  76; Training loss = 0.0251; Validation loss = 0.0153\n",
      "Epoch =  77; Training loss = 0.0234; Validation loss = 0.0172\n",
      "Epoch =  78; Training loss = 0.0241; Validation loss = 0.0150\n",
      "Epoch =  79; Training loss = 0.0228; Validation loss = 0.0152\n",
      "Epoch =  80; Training loss = 0.0219; Validation loss = 0.0154\n",
      "Epoch =  81; Training loss = 0.0228; Validation loss = 0.0152\n",
      "Epoch =  82; Training loss = 0.0214; Validation loss = 0.0143\n",
      "Epoch =  83; Training loss = 0.0212; Validation loss = 0.0137\n",
      "Epoch =  84; Training loss = 0.0209; Validation loss = 0.0156\n",
      "Epoch =  85; Training loss = 0.0202; Validation loss = 0.0159\n",
      "Epoch =  86; Training loss = 0.0212; Validation loss = 0.0138\n",
      "Epoch =  87; Training loss = 0.0205; Validation loss = 0.0146\n",
      "Epoch =  88; Training loss = 0.0199; Validation loss = 0.0149\n",
      "Epoch =  89; Training loss = 0.0203; Validation loss = 0.0149\n",
      "Epoch =  90; Training loss = 0.0202; Validation loss = 0.0148\n",
      "Epoch =  91; Training loss = 0.0198; Validation loss = 0.0147\n",
      "Epoch =  92; Training loss = 0.0181; Validation loss = 0.0147\n",
      "Epoch =  93; Training loss = 0.0173; Validation loss = 0.0090\n",
      "Epoch =  94; Training loss = 0.0191; Validation loss = 0.0142\n",
      "Epoch =  95; Training loss = 0.0170; Validation loss = 0.0138\n",
      "Epoch =  96; Training loss = 0.0175; Validation loss = 0.0146\n",
      "Epoch =  97; Training loss = 0.0174; Validation loss = 0.0146\n",
      "Epoch =  98; Training loss = 0.0178; Validation loss = 0.0140\n",
      "Epoch =  99; Training loss = 0.0162; Validation loss = 0.0151\n",
      "Epoch = 100; Training loss = 0.0174; Validation loss = 0.0136\n",
      "Epoch = 101; Training loss = 0.0163; Validation loss = 0.0140\n",
      "Epoch = 102; Training loss = 0.0168; Validation loss = 0.0097\n",
      "Epoch = 103; Training loss = 0.0172; Validation loss = 0.0147\n",
      "Epoch = 104; Training loss = 0.0164; Validation loss = 0.0124\n",
      "Epoch = 105; Training loss = 0.0164; Validation loss = 0.0131\n",
      "Epoch = 106; Training loss = 0.0162; Validation loss = 0.0082\n",
      "Epoch = 107; Training loss = 0.0161; Validation loss = 0.0118\n",
      "Epoch = 108; Training loss = 0.0148; Validation loss = 0.0132\n",
      "Epoch = 109; Training loss = 0.0147; Validation loss = 0.0134\n",
      "Epoch = 110; Training loss = 0.0149; Validation loss = 0.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 111; Training loss = 0.0157; Validation loss = 0.0135\n",
      "Epoch = 112; Training loss = 0.0151; Validation loss = 0.0117\n",
      "Epoch = 113; Training loss = 0.0147; Validation loss = 0.0137\n",
      "Epoch = 114; Training loss = 0.0149; Validation loss = 0.0133\n",
      "Epoch = 115; Training loss = 0.0145; Validation loss = 0.0085\n",
      "Epoch = 116; Training loss = 0.0146; Validation loss = 0.0125\n",
      "Epoch = 117; Training loss = 0.0145; Validation loss = 0.0127\n",
      "Epoch = 118; Training loss = 0.0141; Validation loss = 0.0123\n",
      "Epoch = 119; Training loss = 0.0134; Validation loss = 0.0127\n",
      "Epoch = 120; Training loss = 0.0138; Validation loss = 0.0138\n",
      "Epoch = 121; Training loss = 0.0134; Validation loss = 0.0135\n",
      "Epoch = 122; Training loss = 0.0137; Validation loss = 0.0132\n",
      "Epoch = 123; Training loss = 0.0136; Validation loss = 0.0118\n",
      "Epoch = 124; Training loss = 0.0121; Validation loss = 0.0120\n",
      "Epoch = 125; Training loss = 0.0129; Validation loss = 0.0121\n",
      "Epoch = 126; Training loss = 0.0130; Validation loss = 0.0124\n",
      "Epoch = 127; Training loss = 0.0121; Validation loss = 0.0128\n",
      "Epoch = 128; Training loss = 0.0127; Validation loss = 0.0129\n",
      "Epoch = 129; Training loss = 0.0121; Validation loss = 0.0072\n",
      "Epoch = 130; Training loss = 0.0119; Validation loss = 0.0117\n",
      "Epoch = 131; Training loss = 0.0128; Validation loss = 0.0123\n",
      "Epoch = 132; Training loss = 0.0126; Validation loss = 0.0127\n",
      "Epoch = 133; Training loss = 0.0121; Validation loss = 0.0127\n",
      "Epoch = 134; Training loss = 0.0126; Validation loss = 0.0135\n",
      "Epoch = 135; Training loss = 0.0122; Validation loss = 0.0127\n",
      "Epoch = 136; Training loss = 0.0113; Validation loss = 0.0122\n",
      "Epoch = 137; Training loss = 0.0119; Validation loss = 0.0119\n",
      "Epoch = 138; Training loss = 0.0112; Validation loss = 0.0130\n",
      "Epoch = 139; Training loss = 0.0117; Validation loss = 0.0127\n",
      "Epoch = 140; Training loss = 0.0102; Validation loss = 0.0133\n",
      "Epoch = 141; Training loss = 0.0116; Validation loss = 0.0110\n",
      "Epoch = 142; Training loss = 0.0113; Validation loss = 0.0123\n",
      "Epoch = 143; Training loss = 0.0111; Validation loss = 0.0128\n",
      "Epoch = 144; Training loss = 0.0104; Validation loss = 0.0131\n",
      "Epoch = 145; Training loss = 0.0110; Validation loss = 0.0135\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5840 of 5846. Accuracy: 99.89736572015053%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    1    0]\n",
      " [   0  958    1    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   1    0    0    2  972    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.01, 500) // Accuracy: 99.89736572015053%\n",
      "Epoch =   0; Training loss = 0.2145; Validation loss = 0.0409\n",
      "Epoch =   1; Training loss = nan; Validation loss = nan\n",
      "Epoch =   2; Training loss = nan; Validation loss = nan\n",
      "Epoch =   3; Training loss = nan; Validation loss = nan\n",
      "Epoch =   4; Training loss = nan; Validation loss = nan\n",
      "Epoch =   5; Training loss = nan; Validation loss = nan\n",
      "Epoch =   6; Training loss = nan; Validation loss = nan\n",
      "Epoch =   7; Training loss = nan; Validation loss = nan\n",
      "Epoch =   8; Training loss = nan; Validation loss = nan\n",
      "Epoch =   9; Training loss = nan; Validation loss = nan\n",
      "Epoch =  10; Training loss = nan; Validation loss = nan\n",
      "Epoch =  11; Training loss = nan; Validation loss = nan\n",
      "Epoch =  12; Training loss = nan; Validation loss = nan\n",
      "Epoch =  13; Training loss = nan; Validation loss = nan\n",
      "Epoch =  14; Training loss = nan; Validation loss = nan\n",
      "Epoch =  15; Training loss = nan; Validation loss = nan\n",
      "Epoch =  16; Training loss = nan; Validation loss = nan\n",
      "Epoch =  17; Training loss = nan; Validation loss = nan\n",
      "Epoch =  18; Training loss = nan; Validation loss = nan\n",
      "Epoch =  19; Training loss = nan; Validation loss = nan\n",
      "Epoch =  20; Training loss = nan; Validation loss = nan\n",
      "Epoch =  21; Training loss = nan; Validation loss = nan\n",
      "Epoch =  22; Training loss = nan; Validation loss = nan\n",
      "Epoch =  23; Training loss = nan; Validation loss = nan\n",
      "Epoch =  24; Training loss = nan; Validation loss = nan\n",
      "Epoch =  25; Training loss = nan; Validation loss = nan\n",
      "Epoch =  26; Training loss = nan; Validation loss = nan\n",
      "Epoch =  27; Training loss = nan; Validation loss = nan\n",
      "Epoch =  28; Training loss = nan; Validation loss = nan\n",
      "Epoch =  29; Training loss = nan; Validation loss = nan\n",
      "Epoch =  30; Training loss = nan; Validation loss = nan\n",
      "Epoch =  31; Training loss = nan; Validation loss = nan\n",
      "Epoch =  32; Training loss = nan; Validation loss = nan\n",
      "Epoch =  33; Training loss = nan; Validation loss = nan\n",
      "Epoch =  34; Training loss = nan; Validation loss = nan\n",
      "Epoch =  35; Training loss = nan; Validation loss = nan\n",
      "Epoch =  36; Training loss = nan; Validation loss = nan\n",
      "Epoch =  37; Training loss = nan; Validation loss = nan\n",
      "Epoch =  38; Training loss = nan; Validation loss = nan\n",
      "Epoch =  39; Training loss = nan; Validation loss = nan\n",
      "Epoch =  40; Training loss = nan; Validation loss = nan\n",
      "Epoch =  41; Training loss = nan; Validation loss = nan\n",
      "Epoch =  42; Training loss = nan; Validation loss = nan\n",
      "Epoch =  43; Training loss = nan; Validation loss = nan\n",
      "Epoch =  44; Training loss = nan; Validation loss = nan\n",
      "Epoch =  45; Training loss = nan; Validation loss = nan\n",
      "Epoch =  46; Training loss = nan; Validation loss = nan\n",
      "Epoch =  47; Training loss = nan; Validation loss = nan\n",
      "Epoch =  48; Training loss = nan; Validation loss = nan\n",
      "Epoch =  49; Training loss = nan; Validation loss = nan\n",
      "Epoch =  50; Training loss = nan; Validation loss = nan\n",
      "Epoch =  51; Training loss = nan; Validation loss = nan\n",
      "Epoch =  52; Training loss = nan; Validation loss = nan\n",
      "Epoch =  53; Training loss = nan; Validation loss = nan\n",
      "Epoch =  54; Training loss = nan; Validation loss = nan\n",
      "Epoch =  55; Training loss = nan; Validation loss = nan\n",
      "Epoch =  56; Training loss = nan; Validation loss = nan\n",
      "Epoch =  57; Training loss = nan; Validation loss = nan\n",
      "Epoch =  58; Training loss = nan; Validation loss = nan\n",
      "Epoch =  59; Training loss = nan; Validation loss = nan\n",
      "Epoch =  60; Training loss = nan; Validation loss = nan\n",
      "Epoch =  61; Training loss = nan; Validation loss = nan\n",
      "Epoch =  62; Training loss = nan; Validation loss = nan\n",
      "Epoch =  63; Training loss = nan; Validation loss = nan\n",
      "Epoch =  64; Training loss = nan; Validation loss = nan\n",
      "Epoch =  65; Training loss = nan; Validation loss = nan\n",
      "Epoch =  66; Training loss = nan; Validation loss = nan\n",
      "Epoch =  67; Training loss = nan; Validation loss = nan\n",
      "Epoch =  68; Training loss = nan; Validation loss = nan\n",
      "Epoch =  69; Training loss = nan; Validation loss = nan\n",
      "Epoch =  70; Training loss = nan; Validation loss = nan\n",
      "Epoch =  71; Training loss = nan; Validation loss = nan\n",
      "Epoch =  72; Training loss = nan; Validation loss = nan\n",
      "Epoch =  73; Training loss = nan; Validation loss = nan\n",
      "Epoch =  74; Training loss = nan; Validation loss = nan\n",
      "Epoch =  75; Training loss = nan; Validation loss = nan\n",
      "Epoch =  76; Training loss = nan; Validation loss = nan\n",
      "Epoch =  77; Training loss = nan; Validation loss = nan\n",
      "Epoch =  78; Training loss = nan; Validation loss = nan\n",
      "Epoch =  79; Training loss = nan; Validation loss = nan\n",
      "Epoch =  80; Training loss = nan; Validation loss = nan\n",
      "Epoch =  81; Training loss = nan; Validation loss = nan\n",
      "Epoch =  82; Training loss = nan; Validation loss = nan\n",
      "Epoch =  83; Training loss = nan; Validation loss = nan\n",
      "Epoch =  84; Training loss = nan; Validation loss = nan\n",
      "Epoch =  85; Training loss = nan; Validation loss = nan\n",
      "Epoch =  86; Training loss = nan; Validation loss = nan\n",
      "Epoch =  87; Training loss = nan; Validation loss = nan\n",
      "Epoch =  88; Training loss = nan; Validation loss = nan\n",
      "Epoch =  89; Training loss = nan; Validation loss = nan\n",
      "Epoch =  90; Training loss = nan; Validation loss = nan\n",
      "Epoch =  91; Training loss = nan; Validation loss = nan\n",
      "Epoch =  92; Training loss = nan; Validation loss = nan\n",
      "Epoch =  93; Training loss = nan; Validation loss = nan\n",
      "Epoch =  94; Training loss = nan; Validation loss = nan\n",
      "Epoch =  95; Training loss = nan; Validation loss = nan\n",
      "Epoch =  96; Training loss = nan; Validation loss = nan\n",
      "Epoch =  97; Training loss = nan; Validation loss = nan\n",
      "Epoch =  98; Training loss = nan; Validation loss = nan\n",
      "Epoch =  99; Training loss = nan; Validation loss = nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100; Training loss = nan; Validation loss = nan\n",
      "Epoch = 101; Training loss = nan; Validation loss = nan\n",
      "Epoch = 102; Training loss = nan; Validation loss = nan\n",
      "Epoch = 103; Training loss = nan; Validation loss = nan\n",
      "Epoch = 104; Training loss = nan; Validation loss = nan\n",
      "Epoch = 105; Training loss = nan; Validation loss = nan\n",
      "Epoch = 106; Training loss = nan; Validation loss = nan\n",
      "Epoch = 107; Training loss = nan; Validation loss = nan\n",
      "Epoch = 108; Training loss = nan; Validation loss = nan\n",
      "Epoch = 109; Training loss = nan; Validation loss = nan\n",
      "Epoch = 110; Training loss = nan; Validation loss = nan\n",
      "Epoch = 111; Training loss = nan; Validation loss = nan\n",
      "Epoch = 112; Training loss = nan; Validation loss = nan\n",
      "Epoch = 113; Training loss = nan; Validation loss = nan\n",
      "Epoch = 114; Training loss = nan; Validation loss = nan\n",
      "Epoch = 115; Training loss = nan; Validation loss = nan\n",
      "Epoch = 116; Training loss = nan; Validation loss = nan\n",
      "Epoch = 117; Training loss = nan; Validation loss = nan\n",
      "Epoch = 118; Training loss = nan; Validation loss = nan\n",
      "Epoch = 119; Training loss = nan; Validation loss = nan\n",
      "Epoch = 120; Training loss = nan; Validation loss = nan\n",
      "Epoch = 121; Training loss = nan; Validation loss = nan\n",
      "Epoch = 122; Training loss = nan; Validation loss = nan\n",
      "Epoch = 123; Training loss = nan; Validation loss = nan\n",
      "Epoch = 124; Training loss = nan; Validation loss = nan\n",
      "Epoch = 125; Training loss = nan; Validation loss = nan\n",
      "Epoch = 126; Training loss = nan; Validation loss = nan\n",
      "Epoch = 127; Training loss = nan; Validation loss = nan\n",
      "Epoch = 128; Training loss = nan; Validation loss = nan\n",
      "Epoch = 129; Training loss = nan; Validation loss = nan\n",
      "Epoch = 130; Training loss = nan; Validation loss = nan\n",
      "Epoch = 131; Training loss = nan; Validation loss = nan\n",
      "Epoch = 132; Training loss = nan; Validation loss = nan\n",
      "Epoch = 133; Training loss = nan; Validation loss = nan\n",
      "Epoch = 134; Training loss = nan; Validation loss = nan\n",
      "Epoch = 135; Training loss = nan; Validation loss = nan\n",
      "Epoch = 136; Training loss = nan; Validation loss = nan\n",
      "Epoch = 137; Training loss = nan; Validation loss = nan\n",
      "Epoch = 138; Training loss = nan; Validation loss = nan\n",
      "Epoch = 139; Training loss = nan; Validation loss = nan\n",
      "Epoch = 140; Training loss = nan; Validation loss = nan\n",
      "Epoch = 141; Training loss = nan; Validation loss = nan\n",
      "Epoch = 142; Training loss = nan; Validation loss = nan\n",
      "Epoch = 143; Training loss = nan; Validation loss = nan\n",
      "Epoch = 144; Training loss = nan; Validation loss = nan\n",
      "Epoch = 145; Training loss = nan; Validation loss = nan\n",
      "Epoch = 146; Training loss = nan; Validation loss = nan\n",
      "Epoch = 147; Training loss = nan; Validation loss = nan\n",
      "Epoch = 148; Training loss = nan; Validation loss = nan\n",
      "Epoch = 149; Training loss = nan; Validation loss = nan\n",
      "Epoch = 150; Training loss = nan; Validation loss = nan\n",
      "Epoch = 151; Training loss = nan; Validation loss = nan\n",
      "Epoch = 152; Training loss = nan; Validation loss = nan\n",
      "Epoch = 153; Training loss = nan; Validation loss = nan\n",
      "Epoch = 154; Training loss = nan; Validation loss = nan\n",
      "Epoch = 155; Training loss = nan; Validation loss = nan\n",
      "Epoch = 156; Training loss = nan; Validation loss = nan\n",
      "Epoch = 157; Training loss = nan; Validation loss = nan\n",
      "Epoch = 158; Training loss = nan; Validation loss = nan\n",
      "Epoch = 159; Training loss = nan; Validation loss = nan\n",
      "Epoch = 160; Training loss = nan; Validation loss = nan\n",
      "Epoch = 161; Training loss = nan; Validation loss = nan\n",
      "Epoch = 162; Training loss = nan; Validation loss = nan\n",
      "Epoch = 163; Training loss = nan; Validation loss = nan\n",
      "Epoch = 164; Training loss = nan; Validation loss = nan\n",
      "Epoch = 165; Training loss = nan; Validation loss = nan\n",
      "Epoch = 166; Training loss = nan; Validation loss = nan\n",
      "Epoch = 167; Training loss = nan; Validation loss = nan\n",
      "Epoch = 168; Training loss = nan; Validation loss = nan\n",
      "Epoch = 169; Training loss = nan; Validation loss = nan\n",
      "Epoch = 170; Training loss = nan; Validation loss = nan\n",
      "Epoch = 171; Training loss = nan; Validation loss = nan\n",
      "Epoch = 172; Training loss = nan; Validation loss = nan\n",
      "Epoch = 173; Training loss = nan; Validation loss = nan\n",
      "Epoch = 174; Training loss = nan; Validation loss = nan\n",
      "Epoch = 175; Training loss = nan; Validation loss = nan\n",
      "Epoch = 176; Training loss = nan; Validation loss = nan\n",
      "Epoch = 177; Training loss = nan; Validation loss = nan\n",
      "Epoch = 178; Training loss = nan; Validation loss = nan\n",
      "Epoch = 179; Training loss = nan; Validation loss = nan\n",
      "Epoch = 180; Training loss = nan; Validation loss = nan\n",
      "Epoch = 181; Training loss = nan; Validation loss = nan\n",
      "Epoch = 182; Training loss = nan; Validation loss = nan\n",
      "Epoch = 183; Training loss = nan; Validation loss = nan\n",
      "Epoch = 184; Training loss = nan; Validation loss = nan\n",
      "Epoch = 185; Training loss = nan; Validation loss = nan\n",
      "Epoch = 186; Training loss = nan; Validation loss = nan\n",
      "Epoch = 187; Training loss = nan; Validation loss = nan\n",
      "Epoch = 188; Training loss = nan; Validation loss = nan\n",
      "Epoch = 189; Training loss = nan; Validation loss = nan\n",
      "Epoch = 190; Training loss = nan; Validation loss = nan\n",
      "Epoch = 191; Training loss = nan; Validation loss = nan\n",
      "Epoch = 192; Training loss = nan; Validation loss = nan\n",
      "Epoch = 193; Training loss = nan; Validation loss = nan\n",
      "Epoch = 194; Training loss = nan; Validation loss = nan\n",
      "Epoch = 195; Training loss = nan; Validation loss = nan\n",
      "Epoch = 196; Training loss = nan; Validation loss = nan\n",
      "Epoch = 197; Training loss = nan; Validation loss = nan\n",
      "Epoch = 198; Training loss = nan; Validation loss = nan\n",
      "Epoch = 199; Training loss = nan; Validation loss = nan\n",
      "Correct predictions: 986 of 5846. Accuracy: 16.866233321929524%\n",
      "Confusion Matrix:\n",
      "[[ 986    0    0    0    0    0]\n",
      " [ 959    0    0    0    0    0]\n",
      " [ 981    0    0    0    0    0]\n",
      " [1013    0    0    0    0    0]\n",
      " [ 975    0    0    0    0    0]\n",
      " [ 932    0    0    0    0    0]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.1, 50) // Accuracy: 16.866233321929524%\n",
      "Epoch =   0; Training loss = 0.2271; Validation loss = 0.0615\n",
      "Epoch =   1; Training loss = 0.0764; Validation loss = 0.0253\n",
      "Epoch =   2; Training loss = 0.0534; Validation loss = 0.0222\n",
      "Epoch =   3; Training loss = 0.0453; Validation loss = 0.0193\n",
      "Epoch =   4; Training loss = 0.0711; Validation loss = 0.0887\n",
      "Epoch =   5; Training loss = 0.0696; Validation loss = 0.0200\n",
      "Epoch =   6; Training loss = 0.0414; Validation loss = 0.0196\n",
      "Epoch =   7; Training loss = 0.0331; Validation loss = 0.0188\n",
      "Epoch =   8; Training loss = 0.0258; Validation loss = 0.0214\n",
      "Epoch =   9; Training loss = 0.0287; Validation loss = 0.0198\n",
      "Epoch =  10; Training loss = 0.0205; Validation loss = 0.0232\n",
      "Epoch =  11; Training loss = 0.0193; Validation loss = 0.0264\n",
      "Epoch =  12; Training loss = 0.0200; Validation loss = 0.0222\n",
      "Epoch =  13; Training loss = 0.0318; Validation loss = 0.0230\n",
      "Epoch =  14; Training loss = 0.0181; Validation loss = 0.0194\n",
      "Epoch =  15; Training loss = 0.0313; Validation loss = 0.0273\n",
      "Epoch =  16; Training loss = 0.0171; Validation loss = 0.0227\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5838 of 5846. Accuracy: 99.86315429353404%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    0    1]\n",
      " [   0  957    2    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1013    0    0]\n",
      " [   2    0    0    2  971    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.1, 100) // Accuracy: 99.86315429353404%\n",
      "Epoch =   0; Training loss = 0.3221; Validation loss = 0.1051\n",
      "Epoch =   1; Training loss = 0.1207; Validation loss = 0.1615\n",
      "Epoch =   2; Training loss = nan; Validation loss = nan\n",
      "Epoch =   3; Training loss = nan; Validation loss = nan\n",
      "Epoch =   4; Training loss = nan; Validation loss = nan\n",
      "Epoch =   5; Training loss = nan; Validation loss = nan\n",
      "Epoch =   6; Training loss = nan; Validation loss = nan\n",
      "Epoch =   7; Training loss = nan; Validation loss = nan\n",
      "Epoch =   8; Training loss = nan; Validation loss = nan\n",
      "Epoch =   9; Training loss = nan; Validation loss = nan\n",
      "Epoch =  10; Training loss = nan; Validation loss = nan\n",
      "Epoch =  11; Training loss = nan; Validation loss = nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  12; Training loss = nan; Validation loss = nan\n",
      "Epoch =  13; Training loss = nan; Validation loss = nan\n",
      "Epoch =  14; Training loss = nan; Validation loss = nan\n",
      "Epoch =  15; Training loss = nan; Validation loss = nan\n",
      "Epoch =  16; Training loss = nan; Validation loss = nan\n",
      "Epoch =  17; Training loss = nan; Validation loss = nan\n",
      "Epoch =  18; Training loss = nan; Validation loss = nan\n",
      "Epoch =  19; Training loss = nan; Validation loss = nan\n",
      "Epoch =  20; Training loss = nan; Validation loss = nan\n",
      "Epoch =  21; Training loss = nan; Validation loss = nan\n",
      "Epoch =  22; Training loss = nan; Validation loss = nan\n",
      "Epoch =  23; Training loss = nan; Validation loss = nan\n",
      "Epoch =  24; Training loss = nan; Validation loss = nan\n",
      "Epoch =  25; Training loss = nan; Validation loss = nan\n",
      "Epoch =  26; Training loss = nan; Validation loss = nan\n",
      "Epoch =  27; Training loss = nan; Validation loss = nan\n",
      "Epoch =  28; Training loss = nan; Validation loss = nan\n",
      "Epoch =  29; Training loss = nan; Validation loss = nan\n",
      "Epoch =  30; Training loss = nan; Validation loss = nan\n",
      "Epoch =  31; Training loss = nan; Validation loss = nan\n",
      "Epoch =  32; Training loss = nan; Validation loss = nan\n",
      "Epoch =  33; Training loss = nan; Validation loss = nan\n",
      "Epoch =  34; Training loss = nan; Validation loss = nan\n",
      "Epoch =  35; Training loss = nan; Validation loss = nan\n",
      "Epoch =  36; Training loss = nan; Validation loss = nan\n",
      "Epoch =  37; Training loss = nan; Validation loss = nan\n",
      "Epoch =  38; Training loss = nan; Validation loss = nan\n",
      "Epoch =  39; Training loss = nan; Validation loss = nan\n",
      "Epoch =  40; Training loss = nan; Validation loss = nan\n",
      "Epoch =  41; Training loss = nan; Validation loss = nan\n",
      "Epoch =  42; Training loss = nan; Validation loss = nan\n",
      "Epoch =  43; Training loss = nan; Validation loss = nan\n",
      "Epoch =  44; Training loss = nan; Validation loss = nan\n",
      "Epoch =  45; Training loss = nan; Validation loss = nan\n",
      "Epoch =  46; Training loss = nan; Validation loss = nan\n",
      "Epoch =  47; Training loss = nan; Validation loss = nan\n",
      "Epoch =  48; Training loss = nan; Validation loss = nan\n",
      "Epoch =  49; Training loss = nan; Validation loss = nan\n",
      "Epoch =  50; Training loss = nan; Validation loss = nan\n",
      "Epoch =  51; Training loss = nan; Validation loss = nan\n",
      "Epoch =  52; Training loss = nan; Validation loss = nan\n",
      "Epoch =  53; Training loss = nan; Validation loss = nan\n",
      "Epoch =  54; Training loss = nan; Validation loss = nan\n",
      "Epoch =  55; Training loss = nan; Validation loss = nan\n",
      "Epoch =  56; Training loss = nan; Validation loss = nan\n",
      "Epoch =  57; Training loss = nan; Validation loss = nan\n",
      "Epoch =  58; Training loss = nan; Validation loss = nan\n",
      "Epoch =  59; Training loss = nan; Validation loss = nan\n",
      "Epoch =  60; Training loss = nan; Validation loss = nan\n",
      "Epoch =  61; Training loss = nan; Validation loss = nan\n",
      "Epoch =  62; Training loss = nan; Validation loss = nan\n",
      "Epoch =  63; Training loss = nan; Validation loss = nan\n",
      "Epoch =  64; Training loss = nan; Validation loss = nan\n",
      "Epoch =  65; Training loss = nan; Validation loss = nan\n",
      "Epoch =  66; Training loss = nan; Validation loss = nan\n",
      "Epoch =  67; Training loss = nan; Validation loss = nan\n",
      "Epoch =  68; Training loss = nan; Validation loss = nan\n",
      "Epoch =  69; Training loss = nan; Validation loss = nan\n",
      "Epoch =  70; Training loss = nan; Validation loss = nan\n",
      "Epoch =  71; Training loss = nan; Validation loss = nan\n",
      "Epoch =  72; Training loss = nan; Validation loss = nan\n",
      "Epoch =  73; Training loss = nan; Validation loss = nan\n",
      "Epoch =  74; Training loss = nan; Validation loss = nan\n",
      "Epoch =  75; Training loss = nan; Validation loss = nan\n",
      "Epoch =  76; Training loss = nan; Validation loss = nan\n",
      "Epoch =  77; Training loss = nan; Validation loss = nan\n",
      "Epoch =  78; Training loss = nan; Validation loss = nan\n",
      "Epoch =  79; Training loss = nan; Validation loss = nan\n",
      "Epoch =  80; Training loss = nan; Validation loss = nan\n",
      "Epoch =  81; Training loss = nan; Validation loss = nan\n",
      "Epoch =  82; Training loss = nan; Validation loss = nan\n",
      "Epoch =  83; Training loss = nan; Validation loss = nan\n",
      "Epoch =  84; Training loss = nan; Validation loss = nan\n",
      "Epoch =  85; Training loss = nan; Validation loss = nan\n",
      "Epoch =  86; Training loss = nan; Validation loss = nan\n",
      "Epoch =  87; Training loss = nan; Validation loss = nan\n",
      "Epoch =  88; Training loss = nan; Validation loss = nan\n",
      "Epoch =  89; Training loss = nan; Validation loss = nan\n",
      "Epoch =  90; Training loss = nan; Validation loss = nan\n",
      "Epoch =  91; Training loss = nan; Validation loss = nan\n",
      "Epoch =  92; Training loss = nan; Validation loss = nan\n",
      "Epoch =  93; Training loss = nan; Validation loss = nan\n",
      "Epoch =  94; Training loss = nan; Validation loss = nan\n",
      "Epoch =  95; Training loss = nan; Validation loss = nan\n",
      "Epoch =  96; Training loss = nan; Validation loss = nan\n",
      "Epoch =  97; Training loss = nan; Validation loss = nan\n",
      "Epoch =  98; Training loss = nan; Validation loss = nan\n",
      "Epoch =  99; Training loss = nan; Validation loss = nan\n",
      "Epoch = 100; Training loss = nan; Validation loss = nan\n",
      "Epoch = 101; Training loss = nan; Validation loss = nan\n",
      "Epoch = 102; Training loss = nan; Validation loss = nan\n",
      "Epoch = 103; Training loss = nan; Validation loss = nan\n",
      "Epoch = 104; Training loss = nan; Validation loss = nan\n",
      "Epoch = 105; Training loss = nan; Validation loss = nan\n",
      "Epoch = 106; Training loss = nan; Validation loss = nan\n",
      "Epoch = 107; Training loss = nan; Validation loss = nan\n",
      "Epoch = 108; Training loss = nan; Validation loss = nan\n",
      "Epoch = 109; Training loss = nan; Validation loss = nan\n",
      "Epoch = 110; Training loss = nan; Validation loss = nan\n",
      "Epoch = 111; Training loss = nan; Validation loss = nan\n",
      "Epoch = 112; Training loss = nan; Validation loss = nan\n",
      "Epoch = 113; Training loss = nan; Validation loss = nan\n",
      "Epoch = 114; Training loss = nan; Validation loss = nan\n",
      "Epoch = 115; Training loss = nan; Validation loss = nan\n",
      "Epoch = 116; Training loss = nan; Validation loss = nan\n",
      "Epoch = 117; Training loss = nan; Validation loss = nan\n",
      "Epoch = 118; Training loss = nan; Validation loss = nan\n",
      "Epoch = 119; Training loss = nan; Validation loss = nan\n",
      "Epoch = 120; Training loss = nan; Validation loss = nan\n",
      "Epoch = 121; Training loss = nan; Validation loss = nan\n",
      "Epoch = 122; Training loss = nan; Validation loss = nan\n",
      "Epoch = 123; Training loss = nan; Validation loss = nan\n",
      "Epoch = 124; Training loss = nan; Validation loss = nan\n",
      "Epoch = 125; Training loss = nan; Validation loss = nan\n",
      "Epoch = 126; Training loss = nan; Validation loss = nan\n",
      "Epoch = 127; Training loss = nan; Validation loss = nan\n",
      "Epoch = 128; Training loss = nan; Validation loss = nan\n",
      "Epoch = 129; Training loss = nan; Validation loss = nan\n",
      "Epoch = 130; Training loss = nan; Validation loss = nan\n",
      "Epoch = 131; Training loss = nan; Validation loss = nan\n",
      "Epoch = 132; Training loss = nan; Validation loss = nan\n",
      "Epoch = 133; Training loss = nan; Validation loss = nan\n",
      "Epoch = 134; Training loss = nan; Validation loss = nan\n",
      "Epoch = 135; Training loss = nan; Validation loss = nan\n",
      "Epoch = 136; Training loss = nan; Validation loss = nan\n",
      "Epoch = 137; Training loss = nan; Validation loss = nan\n",
      "Epoch = 138; Training loss = nan; Validation loss = nan\n",
      "Epoch = 139; Training loss = nan; Validation loss = nan\n",
      "Epoch = 140; Training loss = nan; Validation loss = nan\n",
      "Epoch = 141; Training loss = nan; Validation loss = nan\n",
      "Epoch = 142; Training loss = nan; Validation loss = nan\n",
      "Epoch = 143; Training loss = nan; Validation loss = nan\n",
      "Epoch = 144; Training loss = nan; Validation loss = nan\n",
      "Epoch = 145; Training loss = nan; Validation loss = nan\n",
      "Epoch = 146; Training loss = nan; Validation loss = nan\n",
      "Epoch = 147; Training loss = nan; Validation loss = nan\n",
      "Epoch = 148; Training loss = nan; Validation loss = nan\n",
      "Epoch = 149; Training loss = nan; Validation loss = nan\n",
      "Epoch = 150; Training loss = nan; Validation loss = nan\n",
      "Epoch = 151; Training loss = nan; Validation loss = nan\n",
      "Epoch = 152; Training loss = nan; Validation loss = nan\n",
      "Epoch = 153; Training loss = nan; Validation loss = nan\n",
      "Epoch = 154; Training loss = nan; Validation loss = nan\n",
      "Epoch = 155; Training loss = nan; Validation loss = nan\n",
      "Epoch = 156; Training loss = nan; Validation loss = nan\n",
      "Epoch = 157; Training loss = nan; Validation loss = nan\n",
      "Epoch = 158; Training loss = nan; Validation loss = nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 159; Training loss = nan; Validation loss = nan\n",
      "Epoch = 160; Training loss = nan; Validation loss = nan\n",
      "Epoch = 161; Training loss = nan; Validation loss = nan\n",
      "Epoch = 162; Training loss = nan; Validation loss = nan\n",
      "Epoch = 163; Training loss = nan; Validation loss = nan\n",
      "Epoch = 164; Training loss = nan; Validation loss = nan\n",
      "Epoch = 165; Training loss = nan; Validation loss = nan\n",
      "Epoch = 166; Training loss = nan; Validation loss = nan\n",
      "Epoch = 167; Training loss = nan; Validation loss = nan\n",
      "Epoch = 168; Training loss = nan; Validation loss = nan\n",
      "Epoch = 169; Training loss = nan; Validation loss = nan\n",
      "Epoch = 170; Training loss = nan; Validation loss = nan\n",
      "Epoch = 171; Training loss = nan; Validation loss = nan\n",
      "Epoch = 172; Training loss = nan; Validation loss = nan\n",
      "Epoch = 173; Training loss = nan; Validation loss = nan\n",
      "Epoch = 174; Training loss = nan; Validation loss = nan\n",
      "Epoch = 175; Training loss = nan; Validation loss = nan\n",
      "Epoch = 176; Training loss = nan; Validation loss = nan\n",
      "Epoch = 177; Training loss = nan; Validation loss = nan\n",
      "Epoch = 178; Training loss = nan; Validation loss = nan\n",
      "Epoch = 179; Training loss = nan; Validation loss = nan\n",
      "Epoch = 180; Training loss = nan; Validation loss = nan\n",
      "Epoch = 181; Training loss = nan; Validation loss = nan\n",
      "Epoch = 182; Training loss = nan; Validation loss = nan\n",
      "Epoch = 183; Training loss = nan; Validation loss = nan\n",
      "Epoch = 184; Training loss = nan; Validation loss = nan\n",
      "Epoch = 185; Training loss = nan; Validation loss = nan\n",
      "Epoch = 186; Training loss = nan; Validation loss = nan\n",
      "Epoch = 187; Training loss = nan; Validation loss = nan\n",
      "Epoch = 188; Training loss = nan; Validation loss = nan\n",
      "Epoch = 189; Training loss = nan; Validation loss = nan\n",
      "Epoch = 190; Training loss = nan; Validation loss = nan\n",
      "Epoch = 191; Training loss = nan; Validation loss = nan\n",
      "Epoch = 192; Training loss = nan; Validation loss = nan\n",
      "Epoch = 193; Training loss = nan; Validation loss = nan\n",
      "Epoch = 194; Training loss = nan; Validation loss = nan\n",
      "Epoch = 195; Training loss = nan; Validation loss = nan\n",
      "Epoch = 196; Training loss = nan; Validation loss = nan\n",
      "Epoch = 197; Training loss = nan; Validation loss = nan\n",
      "Epoch = 198; Training loss = nan; Validation loss = nan\n",
      "Epoch = 199; Training loss = nan; Validation loss = nan\n",
      "Correct predictions: 986 of 5846. Accuracy: 16.866233321929524%\n",
      "Confusion Matrix:\n",
      "[[ 986    0    0    0    0    0]\n",
      " [ 959    0    0    0    0    0]\n",
      " [ 981    0    0    0    0    0]\n",
      " [1013    0    0    0    0    0]\n",
      " [ 975    0    0    0    0    0]\n",
      " [ 932    0    0    0    0    0]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.1, 200) // Accuracy: 16.866233321929524%\n",
      "Epoch =   0; Training loss = 0.3374; Validation loss = 0.0722\n",
      "Epoch =   1; Training loss = 0.0879; Validation loss = 0.0354\n",
      "Epoch =   2; Training loss = 0.0586; Validation loss = 0.0221\n",
      "Epoch =   3; Training loss = 0.0431; Validation loss = 0.0148\n",
      "Epoch =   4; Training loss = 0.0342; Validation loss = 0.0148\n",
      "Epoch =   5; Training loss = 0.0264; Validation loss = 0.0109\n",
      "Epoch =   6; Training loss = 0.0297; Validation loss = 0.0176\n",
      "Epoch =   7; Training loss = 0.0251; Validation loss = 0.0107\n",
      "Epoch =   8; Training loss = 0.0176; Validation loss = 0.0126\n",
      "Epoch =   9; Training loss = 0.0179; Validation loss = 0.0103\n",
      "Epoch =  10; Training loss = 0.0151; Validation loss = 0.0101\n",
      "Epoch =  11; Training loss = 0.0129; Validation loss = 0.0107\n",
      "Epoch =  12; Training loss = 0.0138; Validation loss = 0.0093\n",
      "Epoch =  13; Training loss = 0.0139; Validation loss = 0.0113\n",
      "Epoch =  14; Training loss = 0.0108; Validation loss = 0.0089\n",
      "Epoch =  15; Training loss = 0.0078; Validation loss = 0.0081\n",
      "Epoch =  16; Training loss = 0.0117; Validation loss = 0.0107\n",
      "Epoch =  17; Training loss = 0.0093; Validation loss = 0.0076\n",
      "Epoch =  18; Training loss = 0.0077; Validation loss = 0.0096\n",
      "Epoch =  19; Training loss = 0.0074; Validation loss = 0.0085\n",
      "Epoch =  20; Training loss = 0.0091; Validation loss = 0.0108\n",
      "Epoch =  21; Training loss = 0.0073; Validation loss = 0.0108\n",
      "Epoch =  22; Training loss = 0.0080; Validation loss = 0.0087\n",
      "Epoch =  23; Training loss = 0.0075; Validation loss = 0.0087\n",
      "Epoch =  24; Training loss = 0.0072; Validation loss = 0.0083\n",
      "Epoch =  25; Training loss = 0.0058; Validation loss = 0.0093\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5839 of 5846. Accuracy: 99.88026000684228%\n",
      "Confusion Matrix:\n",
      "[[ 985    0    0    0    0    1]\n",
      " [   0  959    0    0    0    0]\n",
      " [   0    1  980    0    0    0]\n",
      " [   0    0    0 1012    1    0]\n",
      " [   3    0    0    1  971    0]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.1, 300) // Accuracy: 99.88026000684228%\n",
      "Epoch =   0; Training loss = 0.7090; Validation loss = 0.1655\n",
      "Epoch =   1; Training loss = 0.2176; Validation loss = 0.1754\n",
      "Epoch =   2; Training loss = 0.1446; Validation loss = 0.0622\n",
      "Epoch =   3; Training loss = 0.1002; Validation loss = 0.1045\n",
      "Epoch =   4; Training loss = 0.0886; Validation loss = 0.0403\n",
      "Epoch =   5; Training loss = 0.0588; Validation loss = 0.0270\n",
      "Epoch =   6; Training loss = 0.0547; Validation loss = 0.0494\n",
      "Epoch =   7; Training loss = 0.0544; Validation loss = 0.0256\n",
      "Epoch =   8; Training loss = 0.0420; Validation loss = 0.0232\n",
      "Epoch =   9; Training loss = 0.0301; Validation loss = 0.0204\n",
      "Epoch =  10; Training loss = 1.6685; Validation loss = 1.8729\n",
      "Epoch =  11; Training loss = 1.9771; Validation loss = 1.7939\n",
      "Epoch =  12; Training loss = 1.8085; Validation loss = 1.7927\n",
      "Epoch =  13; Training loss = 1.7956; Validation loss = 1.7922\n",
      "Epoch =  14; Training loss = 1.7949; Validation loss = 1.7930\n",
      "Epoch =  15; Training loss = 1.7951; Validation loss = 1.7939\n",
      "Epoch =  16; Training loss = 1.7950; Validation loss = 1.7926\n",
      "Epoch =  17; Training loss = 1.7953; Validation loss = 1.7944\n",
      "Epoch =  18; Training loss = 1.7951; Validation loss = 1.7926\n",
      "Epoch =  19; Training loss = 1.7944; Validation loss = 1.7922\n",
      "Epoch =  20; Training loss = 1.7948; Validation loss = 1.7945\n",
      "Epoch =  21; Training loss = 1.7946; Validation loss = 1.7981\n",
      "Epoch =  22; Training loss = 1.7949; Validation loss = 1.7933\n",
      "Epoch =  23; Training loss = 1.7949; Validation loss = 1.7922\n",
      "Epoch =  24; Training loss = 1.7950; Validation loss = 1.7930\n",
      "Epoch =  25; Training loss = 1.7948; Validation loss = 1.7922\n",
      "Epoch =  26; Training loss = 1.7950; Validation loss = 1.7935\n",
      "Epoch =  27; Training loss = 1.7950; Validation loss = 1.7937\n",
      "Epoch =  28; Training loss = 1.7952; Validation loss = 1.7925\n",
      "Epoch =  29; Training loss = 1.7945; Validation loss = 1.7973\n",
      "Epoch =  30; Training loss = 1.7952; Validation loss = 1.7944\n",
      "Epoch =  31; Training loss = 1.7943; Validation loss = 1.7946\n",
      "Epoch =  32; Training loss = 1.7948; Validation loss = 1.7948\n",
      "Epoch =  33; Training loss = 1.7957; Validation loss = 1.7944\n",
      "Epoch =  34; Training loss = 1.7945; Validation loss = 1.7931\n",
      "Epoch =  35; Training loss = 1.7956; Validation loss = 1.7938\n",
      "Epoch =  36; Training loss = 1.8016; Validation loss = 1.7944\n",
      "Epoch =  37; Training loss = 1.7963; Validation loss = 1.7947\n",
      "Epoch =  38; Training loss = 1.7955; Validation loss = 1.7937\n",
      "Epoch =  39; Training loss = 1.7948; Validation loss = 1.7969\n",
      "Epoch =  40; Training loss = 1.7907; Validation loss = 1.7923\n",
      "Epoch =  41; Training loss = 1.7928; Validation loss = 1.7931\n",
      "Epoch =  42; Training loss = 1.7953; Validation loss = 1.7939\n",
      "Epoch =  43; Training loss = 1.7953; Validation loss = 1.7946\n",
      "Epoch =  44; Training loss = 1.7949; Validation loss = 1.7926\n",
      "Epoch =  45; Training loss = 1.7951; Validation loss = 1.7951\n",
      "Epoch =  46; Training loss = 1.7950; Validation loss = 1.7936\n",
      "Epoch =  47; Training loss = 1.7947; Validation loss = 1.7932\n",
      "Epoch =  48; Training loss = 1.7952; Validation loss = 1.7944\n",
      "Epoch =  49; Training loss = 1.7947; Validation loss = 1.7938\n",
      "Epoch =  50; Training loss = 1.7948; Validation loss = 1.7927\n",
      "Epoch =  51; Training loss = 1.7952; Validation loss = 1.7927\n",
      "Epoch =  52; Training loss = 1.7949; Validation loss = 1.7929\n",
      "Epoch =  53; Training loss = 1.7946; Validation loss = 1.7916\n",
      "Epoch =  54; Training loss = 1.7945; Validation loss = 1.7930\n",
      "Epoch =  55; Training loss = 1.7956; Validation loss = 1.7956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  56; Training loss = 1.7948; Validation loss = 1.7922\n",
      "Epoch =  57; Training loss = 1.7949; Validation loss = 1.7929\n",
      "Epoch =  58; Training loss = 1.7947; Validation loss = 1.7951\n",
      "Epoch =  59; Training loss = 1.7946; Validation loss = 1.7951\n",
      "Epoch =  60; Training loss = 1.7950; Validation loss = 1.7924\n",
      "Epoch =  61; Training loss = 1.7951; Validation loss = 1.7941\n",
      "Epoch =  62; Training loss = 1.7949; Validation loss = 1.7950\n",
      "Epoch =  63; Training loss = 1.7950; Validation loss = 1.7944\n",
      "Epoch =  64; Training loss = 1.7948; Validation loss = 1.7926\n",
      "Epoch =  65; Training loss = 1.7950; Validation loss = 1.7918\n",
      "Epoch =  66; Training loss = 1.7949; Validation loss = 1.7936\n",
      "Epoch =  67; Training loss = 1.7949; Validation loss = 1.7921\n",
      "Epoch =  68; Training loss = 1.7946; Validation loss = 1.7919\n",
      "Epoch =  69; Training loss = 1.7954; Validation loss = 1.7927\n",
      "Epoch =  70; Training loss = 1.7945; Validation loss = 1.7922\n",
      "Epoch =  71; Training loss = 1.7950; Validation loss = 1.7929\n",
      "Epoch =  72; Training loss = 1.7946; Validation loss = 1.7921\n",
      "Epoch =  73; Training loss = 1.7946; Validation loss = 1.7943\n",
      "Epoch =  74; Training loss = 1.7947; Validation loss = 1.7930\n",
      "Epoch =  75; Training loss = 1.7948; Validation loss = 1.7937\n",
      "Epoch =  76; Training loss = 1.7949; Validation loss = 1.7925\n",
      "Epoch =  77; Training loss = 1.7951; Validation loss = 1.7948\n",
      "Epoch =  78; Training loss = 1.7948; Validation loss = 1.7956\n",
      "Epoch =  79; Training loss = 1.7944; Validation loss = 1.7930\n",
      "Epoch =  80; Training loss = 1.7950; Validation loss = 1.7932\n",
      "Epoch =  81; Training loss = 1.7952; Validation loss = 1.7939\n",
      "Epoch =  82; Training loss = 1.7955; Validation loss = 1.7925\n",
      "Epoch =  83; Training loss = 1.7944; Validation loss = 1.7937\n",
      "Epoch =  84; Training loss = 1.7951; Validation loss = 1.7928\n",
      "Epoch =  85; Training loss = 1.7945; Validation loss = 1.7923\n",
      "Epoch =  86; Training loss = 1.7953; Validation loss = 1.7969\n",
      "Epoch =  87; Training loss = 1.7951; Validation loss = 1.7938\n",
      "Epoch =  88; Training loss = 1.7947; Validation loss = 1.7926\n",
      "Epoch =  89; Training loss = 1.7950; Validation loss = 1.7971\n",
      "Epoch =  90; Training loss = 1.7953; Validation loss = 1.7926\n",
      "Epoch =  91; Training loss = 1.7950; Validation loss = 1.7933\n",
      "Epoch =  92; Training loss = 1.7948; Validation loss = 1.7949\n",
      "Epoch =  93; Training loss = 1.7950; Validation loss = 1.7930\n",
      "Epoch =  94; Training loss = 1.7947; Validation loss = 1.7957\n",
      "Epoch =  95; Training loss = 1.7949; Validation loss = 1.7941\n",
      "Epoch =  96; Training loss = 1.7945; Validation loss = 1.7931\n",
      "Epoch =  97; Training loss = 1.7947; Validation loss = 1.7955\n",
      "Epoch =  98; Training loss = 1.7950; Validation loss = 1.7930\n",
      "Epoch =  99; Training loss = 1.7950; Validation loss = 1.7952\n",
      "Epoch = 100; Training loss = 1.7948; Validation loss = 1.7931\n",
      "Epoch = 101; Training loss = 1.7949; Validation loss = 1.7926\n",
      "Epoch = 102; Training loss = 1.7951; Validation loss = 1.7917\n",
      "Epoch = 103; Training loss = 1.7948; Validation loss = 1.7966\n",
      "Epoch = 104; Training loss = 1.7949; Validation loss = 1.7925\n",
      "Epoch = 105; Training loss = 1.7947; Validation loss = 1.7943\n",
      "Epoch = 106; Training loss = 1.7949; Validation loss = 1.7931\n",
      "Epoch = 107; Training loss = 1.7949; Validation loss = 1.7921\n",
      "Epoch = 108; Training loss = 1.7948; Validation loss = 1.7957\n",
      "Epoch = 109; Training loss = 1.7952; Validation loss = 1.7918\n",
      "Epoch = 110; Training loss = 1.7949; Validation loss = 1.7928\n",
      "Epoch = 111; Training loss = 1.7948; Validation loss = 1.7940\n",
      "Epoch = 112; Training loss = 1.7949; Validation loss = 1.7959\n",
      "Epoch = 113; Training loss = 1.7951; Validation loss = 1.7930\n",
      "Epoch = 114; Training loss = 1.7947; Validation loss = 1.7969\n",
      "Epoch = 115; Training loss = 1.7949; Validation loss = 1.7952\n",
      "Epoch = 116; Training loss = 1.7947; Validation loss = 1.7919\n",
      "Epoch = 117; Training loss = 1.7951; Validation loss = 1.7928\n",
      "Epoch = 118; Training loss = 1.7945; Validation loss = 1.7926\n",
      "Epoch = 119; Training loss = 1.7948; Validation loss = 1.7932\n",
      "Epoch = 120; Training loss = 1.7951; Validation loss = 1.7968\n",
      "Epoch = 121; Training loss = 1.7948; Validation loss = 1.7948\n",
      "Epoch = 122; Training loss = 1.7952; Validation loss = 1.7940\n",
      "Epoch = 123; Training loss = 1.7949; Validation loss = 1.7936\n",
      "Epoch = 124; Training loss = 1.7952; Validation loss = 1.7935\n",
      "Epoch = 125; Training loss = 1.7947; Validation loss = 1.7934\n",
      "Epoch = 126; Training loss = 1.7948; Validation loss = 1.7940\n",
      "Epoch = 127; Training loss = 1.7946; Validation loss = 1.7924\n",
      "Epoch = 128; Training loss = 1.7943; Validation loss = 1.7936\n",
      "Epoch = 129; Training loss = 1.7946; Validation loss = 1.7941\n",
      "Epoch = 130; Training loss = 1.7950; Validation loss = 1.7930\n",
      "Epoch = 131; Training loss = 1.7947; Validation loss = 1.7918\n",
      "Epoch = 132; Training loss = 1.7953; Validation loss = 1.7940\n",
      "Epoch = 133; Training loss = 1.7944; Validation loss = 1.7930\n",
      "Epoch = 134; Training loss = 1.7950; Validation loss = 1.7924\n",
      "Epoch = 135; Training loss = 1.7949; Validation loss = 1.7923\n",
      "Epoch = 136; Training loss = 1.7945; Validation loss = 1.7923\n",
      "Epoch = 137; Training loss = 1.7949; Validation loss = 1.7933\n",
      "Epoch = 138; Training loss = 1.7950; Validation loss = 1.7922\n",
      "Epoch = 139; Training loss = 1.7945; Validation loss = 1.7919\n",
      "Epoch = 140; Training loss = 1.7948; Validation loss = 1.7936\n",
      "Epoch = 141; Training loss = 1.7952; Validation loss = 1.7923\n",
      "Epoch = 142; Training loss = 1.7944; Validation loss = 1.7947\n",
      "Epoch = 143; Training loss = 1.7947; Validation loss = 1.7930\n",
      "Epoch = 144; Training loss = 1.7947; Validation loss = 1.7950\n",
      "Epoch = 145; Training loss = 1.7946; Validation loss = 1.7930\n",
      "Epoch = 146; Training loss = 1.7948; Validation loss = 1.7926\n",
      "Epoch = 147; Training loss = 1.7942; Validation loss = 1.7944\n",
      "Epoch = 148; Training loss = 1.7950; Validation loss = 1.7948\n",
      "Epoch = 149; Training loss = 1.7948; Validation loss = 1.7933\n",
      "Epoch = 150; Training loss = 1.7949; Validation loss = 1.7946\n",
      "Epoch = 151; Training loss = 1.7945; Validation loss = 1.7934\n",
      "Epoch = 152; Training loss = 1.7953; Validation loss = 1.7935\n",
      "Epoch = 153; Training loss = 1.7950; Validation loss = 1.7938\n",
      "Epoch = 154; Training loss = 1.7947; Validation loss = 1.7920\n",
      "Epoch = 155; Training loss = 1.7951; Validation loss = 1.7920\n",
      "Epoch = 156; Training loss = 1.7954; Validation loss = 1.7951\n",
      "Epoch = 157; Training loss = 1.7945; Validation loss = 1.7936\n",
      "Epoch = 158; Training loss = 1.7950; Validation loss = 1.7945\n",
      "Epoch = 159; Training loss = 1.7950; Validation loss = 1.7922\n",
      "Epoch = 160; Training loss = 1.7950; Validation loss = 1.7953\n",
      "Epoch = 161; Training loss = 1.7951; Validation loss = 1.7923\n",
      "Epoch = 162; Training loss = 1.7949; Validation loss = 1.7955\n",
      "Epoch = 163; Training loss = 1.7944; Validation loss = 1.7933\n",
      "Epoch = 164; Training loss = 1.7950; Validation loss = 1.7929\n",
      "Epoch = 165; Training loss = 1.7953; Validation loss = 1.7937\n",
      "Epoch = 166; Training loss = 1.7946; Validation loss = 1.7931\n",
      "Epoch = 167; Training loss = 1.7949; Validation loss = 1.7923\n",
      "Epoch = 168; Training loss = 1.7950; Validation loss = 1.7979\n",
      "Epoch = 169; Training loss = 1.7952; Validation loss = 1.7938\n",
      "Epoch = 170; Training loss = 1.7948; Validation loss = 1.7924\n",
      "Epoch = 171; Training loss = 1.7950; Validation loss = 1.7943\n",
      "Epoch = 172; Training loss = 1.7948; Validation loss = 1.7930\n",
      "Epoch = 173; Training loss = 1.7945; Validation loss = 1.7937\n",
      "Epoch = 174; Training loss = 1.7947; Validation loss = 1.7986\n",
      "Epoch = 175; Training loss = 1.7949; Validation loss = 1.7925\n",
      "Epoch = 176; Training loss = 1.7948; Validation loss = 1.7939\n",
      "Epoch = 177; Training loss = 1.7955; Validation loss = 1.7943\n",
      "Epoch = 178; Training loss = 1.7949; Validation loss = 1.7936\n",
      "Epoch = 179; Training loss = 1.7943; Validation loss = 1.7938\n",
      "Epoch = 180; Training loss = 1.7949; Validation loss = 1.7958\n",
      "Epoch = 181; Training loss = 1.7950; Validation loss = 1.7924\n",
      "Epoch = 182; Training loss = 1.7947; Validation loss = 1.7937\n",
      "Epoch = 183; Training loss = 1.7946; Validation loss = 1.7934\n",
      "Epoch = 184; Training loss = 1.7954; Validation loss = 1.7977\n",
      "Epoch = 185; Training loss = 1.7946; Validation loss = 1.7928\n",
      "Epoch = 186; Training loss = 1.7947; Validation loss = 1.7935\n",
      "Epoch = 187; Training loss = 1.7950; Validation loss = 1.7938\n",
      "Epoch = 188; Training loss = 1.7949; Validation loss = 1.7929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 189; Training loss = 1.7945; Validation loss = 1.7927\n",
      "Epoch = 190; Training loss = 1.7949; Validation loss = 1.7953\n",
      "Epoch = 191; Training loss = 1.7951; Validation loss = 1.7945\n",
      "Epoch = 192; Training loss = 1.7948; Validation loss = 1.7922\n",
      "Epoch = 193; Training loss = 1.7948; Validation loss = 1.7960\n",
      "Epoch = 194; Training loss = 1.7945; Validation loss = 1.7928\n",
      "Epoch = 195; Training loss = 1.7946; Validation loss = 1.7967\n",
      "Epoch = 196; Training loss = 1.7946; Validation loss = 1.7934\n",
      "Epoch = 197; Training loss = 1.7948; Validation loss = 1.7921\n",
      "Epoch = 198; Training loss = 1.7946; Validation loss = 1.7933\n",
      "Epoch = 199; Training loss = 1.7947; Validation loss = 1.7939\n",
      "Correct predictions: 932 of 5846. Accuracy: 15.942524803284297%\n",
      "Confusion Matrix:\n",
      "[[   0    0    0    0    0  986]\n",
      " [   0    0    0    0    0  959]\n",
      " [   0    0    0    0    0  981]\n",
      " [   0    0    0    0    0 1013]\n",
      " [   0    0    0    0    0  975]\n",
      " [   0    0    0    0    0  932]]\n",
      "['CXR', 'ChestCT', 'AbdomenCT', 'HeadCT', 'Hand', 'BreastMRI']\n",
      "Parameters: (0.1, 500) // Accuracy: 15.942524803284297%\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [50, 100, 200, 300, 500]\n",
    "results = []\n",
    "for lr in learning_rates:\n",
    "    for batch in batch_sizes:\n",
    "        res = train_model(best_config, \n",
    "                          trainX, \n",
    "                          trainY, \n",
    "                          validX, \n",
    "                          validY, \n",
    "                          testX, \n",
    "                          testY,\n",
    "                          learnRate = lr,\n",
    "                          batchSize = batch,\n",
    "                          verbose=True, \n",
    "                          add_dropout=True)\n",
    "        results.append(((lr, batch), res[1]))\n",
    "        print(f\"Parameters: {(lr, batch)} // Accuracy: {res*100}%\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a81a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 200) // Accuracy: 0.9993157714676703\n",
      "(0.01, 500) // Accuracy: 0.9989736572015053\n",
      "(0.001, 50) // Accuracy: 0.9988026000684228\n",
      "(0.01, 50) // Accuracy: 0.9988026000684228\n",
      "(0.01, 100) // Accuracy: 0.9988026000684228\n",
      "(0.1, 300) // Accuracy: 0.9988026000684228\n",
      "(0.001, 100) // Accuracy: 0.9986315429353404\n",
      "(0.1, 100) // Accuracy: 0.9986315429353404\n",
      "(0.01, 300) // Accuracy: 0.998460485802258\n",
      "(0.001, 200) // Accuracy: 0.9981183715360931\n",
      "(0.001, 500) // Accuracy: 0.9923024290112897\n",
      "(0.001, 300) // Accuracy: 0.99161820047896\n",
      "(0.1, 50) // Accuracy: 0.16866233321929525\n",
      "(0.1, 200) // Accuracy: 0.16866233321929525\n",
      "(0.1, 500) // Accuracy: 0.15942524803284297\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"configuration\", \"accuracy\"])\n",
    "results_df.sort_values(by=\"accuracy\", inplace=True, ascending=False)\n",
    "for index, row in results_df.iterrows():\n",
    "    print(f\"{row['configuration']} // Accuracy: {row['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37156d1a",
   "metadata": {},
   "source": [
    "On constate que notre modèle est plus performant avec un learning rate de 0.01 et une batch size de 200. Enregistrons donc notre modèle entraîné avec ces paramètres sur le disque dur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860b6265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =   0; Training loss = 0.8580; Validation loss = 0.2725\n",
      "Epoch =   1; Training loss = 0.2852; Validation loss = 0.1540\n",
      "Epoch =   2; Training loss = 0.1908; Validation loss = 0.0989\n",
      "Epoch =   3; Training loss = 0.1436; Validation loss = 0.0745\n",
      "Epoch =   4; Training loss = 0.1133; Validation loss = 0.0587\n",
      "Epoch =   5; Training loss = 0.0920; Validation loss = 0.0464\n",
      "Epoch =   6; Training loss = 0.0787; Validation loss = 0.0365\n",
      "Epoch =   7; Training loss = 0.0673; Validation loss = 0.0308\n",
      "Epoch =   8; Training loss = 0.0566; Validation loss = 0.0249\n",
      "Epoch =   9; Training loss = 0.0507; Validation loss = 0.0220\n",
      "Epoch =  10; Training loss = 0.0452; Validation loss = 0.0195\n",
      "Epoch =  11; Training loss = 0.0423; Validation loss = 0.0177\n",
      "Epoch =  12; Training loss = 0.0378; Validation loss = 0.0177\n",
      "Epoch =  13; Training loss = 0.0357; Validation loss = 0.0145\n",
      "Epoch =  14; Training loss = 0.0312; Validation loss = 0.0134\n",
      "Epoch =  15; Training loss = 0.0310; Validation loss = 0.0129\n",
      "Epoch =  16; Training loss = 0.0283; Validation loss = 0.0123\n",
      "Epoch =  17; Training loss = 0.0262; Validation loss = 0.0116\n",
      "Epoch =  18; Training loss = 0.0246; Validation loss = 0.0101\n",
      "Epoch =  19; Training loss = 0.0237; Validation loss = 0.0108\n",
      "Epoch =  20; Training loss = 0.0216; Validation loss = 0.0094\n",
      "Epoch =  21; Training loss = 0.0216; Validation loss = 0.0102\n",
      "Epoch =  22; Training loss = 0.0198; Validation loss = 0.0086\n",
      "Epoch =  23; Training loss = 0.0187; Validation loss = 0.0089\n",
      "Epoch =  24; Training loss = 0.0180; Validation loss = 0.0085\n",
      "Epoch =  25; Training loss = 0.0179; Validation loss = 0.0065\n",
      "Epoch =  26; Training loss = 0.0159; Validation loss = 0.0077\n",
      "Epoch =  27; Training loss = 0.0150; Validation loss = 0.0076\n",
      "Epoch =  28; Training loss = 0.0156; Validation loss = 0.0076\n",
      "Epoch =  29; Training loss = 0.0149; Validation loss = 0.0070\n",
      "Epoch =  30; Training loss = 0.0149; Validation loss = 0.0068\n",
      "Epoch =  31; Training loss = 0.0131; Validation loss = 0.0072\n",
      "Epoch =  32; Training loss = 0.0121; Validation loss = 0.0069\n",
      "Epoch =  33; Training loss = 0.0131; Validation loss = 0.0066\n",
      "Epoch =  34; Training loss = 0.0125; Validation loss = 0.0073\n",
      "Epoch =  35; Training loss = 0.0122; Validation loss = 0.0065\n",
      "Epoch =  36; Training loss = 0.0112; Validation loss = 0.0059\n",
      "Epoch =  37; Training loss = 0.0112; Validation loss = 0.0064\n",
      "Epoch =  38; Training loss = 0.0110; Validation loss = 0.0061\n",
      "Epoch =  39; Training loss = 0.0107; Validation loss = 0.0058\n",
      "Epoch =  40; Training loss = 0.0107; Validation loss = 0.0054\n",
      "Epoch =  41; Training loss = 0.0095; Validation loss = 0.0067\n",
      "Epoch =  42; Training loss = 0.0100; Validation loss = 0.0066\n",
      "Epoch =  43; Training loss = 0.0087; Validation loss = 0.0059\n",
      "Epoch =  44; Training loss = 0.0090; Validation loss = 0.0058\n",
      "Epoch =  45; Training loss = 0.0097; Validation loss = 0.0060\n",
      "Epoch =  46; Training loss = 0.0091; Validation loss = 0.0060\n",
      "Epoch =  47; Training loss = 0.0093; Validation loss = 0.0058\n",
      "Epoch =  48; Training loss = 0.0088; Validation loss = 0.0056\n",
      "Epoch =  49; Training loss = 0.0088; Validation loss = 0.0051\n",
      "Epoch =  50; Training loss = 0.0076; Validation loss = 0.0059\n",
      "Epoch =  51; Training loss = 0.0071; Validation loss = 0.0054\n",
      "Epoch =  52; Training loss = 0.0068; Validation loss = 0.0058\n",
      "Epoch =  53; Training loss = 0.0074; Validation loss = 0.0060\n",
      "Epoch =  54; Training loss = 0.0069; Validation loss = 0.0055\n",
      "Epoch =  55; Training loss = 0.0076; Validation loss = 0.0050\n",
      "Epoch =  56; Training loss = 0.0064; Validation loss = 0.0048\n",
      "Epoch =  57; Training loss = 0.0066; Validation loss = 0.0055\n",
      "Epoch =  58; Training loss = 0.0063; Validation loss = 0.0053\n",
      "Epoch =  59; Training loss = 0.0064; Validation loss = 0.0063\n",
      "Epoch =  60; Training loss = 0.0058; Validation loss = 0.0061\n",
      "Epoch =  61; Training loss = 0.0061; Validation loss = 0.0058\n",
      "Epoch =  62; Training loss = 0.0070; Validation loss = 0.0052\n",
      "Epoch =  63; Training loss = 0.0062; Validation loss = 0.0056\n",
      "Epoch =  64; Training loss = 0.0061; Validation loss = 0.0039\n",
      "Epoch =  65; Training loss = 0.0064; Validation loss = 0.0058\n",
      "Epoch =  66; Training loss = 0.0061; Validation loss = 0.0064\n",
      "Epoch =  67; Training loss = 0.0062; Validation loss = 0.0053\n",
      "Epoch =  68; Training loss = 0.0060; Validation loss = 0.0051\n",
      "Epoch =  69; Training loss = 0.0056; Validation loss = 0.0082\n",
      "Epoch =  70; Training loss = 0.0052; Validation loss = 0.0053\n",
      "Epoch =  71; Training loss = 0.0059; Validation loss = 0.0047\n",
      "Epoch =  72; Training loss = 0.0060; Validation loss = 0.0053\n",
      "Epoch =  73; Training loss = 0.0058; Validation loss = 0.0057\n",
      "Epoch =  74; Training loss = 0.0054; Validation loss = 0.0059\n",
      "Epoch =  75; Training loss = 0.0051; Validation loss = 0.0054\n",
      "Epoch =  76; Training loss = 0.0052; Validation loss = 0.0054\n",
      "Epoch =  77; Training loss = 0.0048; Validation loss = 0.0056\n",
      "Epoch =  78; Training loss = 0.0046; Validation loss = 0.0059\n",
      "Epoch =  79; Training loss = 0.0051; Validation loss = 0.0059\n",
      "Epoch =  80; Training loss = 0.0046; Validation loss = 0.0056\n",
      "Validation loss too high; halting to prevent overfitting\n",
      "Correct predictions: 5840 of 5846. Accuracy: 99.89736572015053%\n",
      "Confusion Matrix:\n",
      "[[ 986    0    0    0    0    0]\n",
      " [   0  958    0    1    0    0]\n",
      " [   0    0  871    0    0    0]\n",
      " [   0    3    1 1031    1    0]\n",
      " [   0    0    0    0  973    0]\n",
      " [   0    0    0    0    0 1021]]\n",
      "['ChestCT', 'CXR', 'BreastMRI', 'Hand', 'HeadCT', 'AbdomenCT']\n"
     ]
    }
   ],
   "source": [
    "# On sauvegarde notre meilleur modèle\n",
    "import pickle\n",
    "best_lr = 0.01\n",
    "best_batch_size = 200\n",
    "res = train_model(best_config, \n",
    "                  trainX, \n",
    "                  trainY, \n",
    "                  validX, \n",
    "                  validY, \n",
    "                  testX, \n",
    "                  testY,\n",
    "                  learnRate = best_lr,\n",
    "                  batchSize = best_batch_size,\n",
    "                  verbose=True, \n",
    "                  add_dropout=True)\n",
    "pickle.dump(res[0], open(\"best_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
